{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196240a0-c91e-4b29-a1a9-98945daf0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4a51be-6387-4dac-9e65-ec17b85af553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# This function uses the pushshift api to connect to a given subreddit and return a set of posts.\n",
    "#\n",
    "# Inputs: 1) Name of the subreddit\n",
    "#         2) Push shift api URL for the subreddit, e.g. 'https://api.pushshift.io/reddit/search/submission'\n",
    "#         3) The number of posts that should be scraped from the subreddit (maximum is 100).\n",
    "#         \n",
    "#  The optional posts_before_time input is typically used when calling this function in a loop, to ensure that\n",
    "#  the posts returned are farther back in time than the oldest post in the last batch.\n",
    "# ================================================================================================================\n",
    "def get_reddit_data(subreddit_name, url, num_posts, posts_before_time = 0, verbose=False): \n",
    "    \n",
    "    # Params specified per the pushshift api to specify which subreddit and the\n",
    "    # number of posts that should be returned.\n",
    "    params = {'subreddit' : subreddit_name,\n",
    "              'size' : num_posts}\n",
    "    \n",
    "    # Add an additional key value pair to the params dictionary if posts_before_time is specified.\n",
    "    if posts_before_time != 0:\n",
    "        params['before'] = posts_before_time\n",
    "    \n",
    "    # Requests posts\n",
    "    res = requests.get(url, params)\n",
    "    \n",
    "    # Save status_code associated with the request\n",
    "    status_code = res.status_code\n",
    "    \n",
    "    # If displaying messages is desired.\n",
    "    if verbose:\n",
    "        print(\"===============================================================================\")\n",
    "        print(f\"Connection to subreddit {subreddit_name} returned status code {status_code}.\")\n",
    "        print(\"===============================================================================\")\n",
    "    \n",
    "    # Check to see if the request was successful (status 200). \n",
    "    # If request was unsuccessful, print an error message and return -1.\n",
    "    # If request was successful, return the json with the post information.\n",
    "    if status_code != 200:\n",
    "        print(f\"Exiting function due to invalid status code ---> {status_code}\")\n",
    "        return -1\n",
    "    else:\n",
    "        return res.json()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84d2704-8710-4fe5-95b6-f8f316764b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# This is a helper function used to convert the json that is returned from a successful call to requests.get()\n",
    "# into a pandas dataframe. \n",
    "# ================================================================================================================\n",
    "def posts_to_dataframe(data, verbose=False, posts_left=None, total_posts=None, name=None):\n",
    "    \n",
    "    # List of dictionaries containing the reddit post information.\n",
    "    posts = data['data']\n",
    "    \n",
    "    # Save the 'created_utc' information from the last post returned.\n",
    "    # This will be use to specify where the next scrape should begin.\n",
    "    last_post_time = posts[-1]['created_utc']\n",
    "    \n",
    "    # If displaying messages is desired.\n",
    "    if verbose:\n",
    "        print(\"\\n==============================================================================\")\n",
    "        print(f\"Successfully obtained {total_posts - posts_left} of {total_posts} posts so far in subreddit {name}.\")\n",
    "        print(f\"First post in this batch created at UTC: {posts[0]['created_utc']}\")\n",
    "        print(f\"Last post in this batch created at UTC: {posts[-1]['created_utc']}\")\n",
    "        print(f\"There are {posts_left} more posts to collect.\")\n",
    "        print(\"==============================================================================\\n\")\n",
    "    \n",
    "    # Convert the list of dictionaries to a pandas dataframe.\n",
    "    df = pd.DataFrame(posts)\n",
    "                      \n",
    "    return df, last_post_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59184746-7dab-4ab6-a72a-0025e8be77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# When scraping posts in a loop for long periods of time, it may be useful to periodically save the data to a\n",
    "# .csv. This helps provide confidence that the script is running as intended, and allows some data to be utilized\n",
    "# before the data collection process has finished.\n",
    "#\n",
    "# ================================================================================================================\n",
    "def posts_to_csv(df, posts_left, total_posts, subreddit, intermediate=True):\n",
    "    \n",
    "    # If this is an intermediate (checkpoint) save, specify the filename accordingly and perform the save.\n",
    "    if intermediate: \n",
    "        \n",
    "        posts_completed = total_posts - posts_left\n",
    "        \n",
    "        df.to_csv(f\"./data/Intermediate/Intermediate_{subreddit}_{posts_completed}_of_{total_posts}_post_data.csv\", index=False)\n",
    "    \n",
    "    # Otherwise this is the final save. Indicate this in the filename and perform the save.\n",
    "    else: \n",
    "        \n",
    "        df.to_csv(f\"./data/Final/Final_{subreddit}_data_{total_posts}_posts.csv\", index=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8946285b-9bcd-4156-8baa-31fd8c13e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# When scraping posts in a loop for long periods of time, there are likely going to be some intermittent failures  \n",
    "# to connect, i.e. requests.get() not returning status 200. This function helps prevent the data collection process\n",
    "# from being disrupted due to this intermittent errors.\n",
    "#\n",
    "# If an attempt to collect posts fails, this function will try up to 5 more times to reconnect. There is a 5 second\n",
    "# waiting period between each reconnection attempt.\n",
    "#\n",
    "# If any reconnection attempt is successful, the data collection process continues as normal. If after five attempts\n",
    "# there has not been a successful connection, the function returns -1, which will cause the data collection\n",
    "# process to halt. \n",
    "# ================================================================================================================\n",
    "def retry_reddit_scrape(subreddit_name, url, posts_per_request, posts_before_time):\n",
    "    \n",
    "    # Cool down period between each reconnection attempt\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Track the number of times we have tried to reconnect.\n",
    "    attempts = 1\n",
    "    \n",
    "    # Try up to 5 times to reconnect. \n",
    "    while attempts < 6:\n",
    "        \n",
    "        print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(f\"Reconnection attempt number {attempts} after a 5 second wait...\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        \n",
    "        # Reconnection attempt\n",
    "        data = get_reddit_data(subreddit_name=subreddit_name, url=url, num_posts=posts_per_request,\n",
    "                               posts_before_time = posts_before_time, verbose=True)\n",
    "        \n",
    "        # If the reconnection attempt was successful, return the data that was collected.\n",
    "        if data != -1:\n",
    "            return data\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a2ec53-3e97-437f-948e-60e971d724fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# This function is used to scrape a specified number of posts from a given subreddit. \n",
    "#\n",
    "# Optional parameters warm, before and start_file are used to add some extra flexibility for situations where\n",
    "# there is a desire to start the post collection from a specified time frame, and append the data collected to\n",
    "# a specified file. \n",
    "# ================================================================================================================\n",
    "def scrape_reddit(subreddit_name = 'CryptoCurrency', url='https://api.pushshift.io/reddit/search/submission', posts_per_request=100, total_posts=2000,\n",
    "                  warm=False, before=None, start_file=None, posts_completed=None, save_every=1000):\n",
    "    \n",
    "    # If we have already collected some of the initially desired posts, and are now rerunning the function\n",
    "    # to pick up where the last round of data collection left off. \n",
    "    if warm:\n",
    "        df = pd.read_csv(start_file)\n",
    "        posts_left = total_posts - posts_completed\n",
    "        last_post_time = before\n",
    "        \n",
    "        print(f\"warm start...{posts_completed} posts have been scraped already, {posts_left} more to go\")\n",
    "    \n",
    "    # If this is not a \"warm\" start, we do not have a file to begin appending our posts to. In this case\n",
    "    # we will collect one batch of posts and store them in a dataframe. This dataframe can then be added to \n",
    "    # in the while loop as additional batches of posts are collected.\n",
    "    else:\n",
    "        \n",
    "        # Get a set of posts\n",
    "        data = get_reddit_data(subreddit_name, url, posts_per_request)\n",
    "        \n",
    "        # If the very first batch of posts we try to collect fails, print an error and return\n",
    "        if data == -1: \n",
    "            print(f\"Exiting {subreddit_name} reddit scrape due to invalid response code after collecting 0 posts\")\n",
    "            return -1\n",
    "        \n",
    "        # Create a dataframe and store the timestamp from the last post collected.\n",
    "        df, last_post_time = posts_to_dataframe(data)\n",
    "        \n",
    "        # decrease the number of posts left to collect.\n",
    "        posts_left = total_posts - posts_per_request\n",
    "    \n",
    "    while posts_left > 0:\n",
    "        \n",
    "        # Rest for 1 second so we don't ping the server too often\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Save the dataframe to a csv every 500 posts scraped, this way if an error occurs we have\n",
    "        # a backup copy of the data that has been collected. \n",
    "        if posts_left % save_every == 0: \n",
    "            posts_to_csv(df, posts_left, total_posts, subreddit_name)\n",
    "            verbose=True\n",
    "        else:\n",
    "            verbose=False\n",
    "            \n",
    "        # Connect to the subreddit and return the json for the last set of posts as a list of python dictionaries\n",
    "        data = get_reddit_data(subreddit_name, url, posts_per_request, posts_before_time = last_post_time)\n",
    "        \n",
    "        # If we were unable to connect to the reddit page...\n",
    "        if data == -1:\n",
    "            \n",
    "            # Try to reconnect up to five times, with a 5 second delay between each attempt.\n",
    "            data = retry_reddit_scrape(subreddit_name, url, posts_per_request, posts_before_time = last_post_time)\n",
    "            \n",
    "            # If none of the retry attempts were successful, save the data we have so far and return. \n",
    "            if data == -1:\n",
    "                print(f\"Failed to reconnect with a valid status code, exiting the {subreddit_name} call to scrape_redit\")\n",
    "                new_df, last_post_time = posts_to_dataframe(data, verbose=verbose, posts_left=posts_left, total_posts=total_posts, name=subreddit_name)\n",
    "                return df\n",
    "            \n",
    "        \n",
    "        # Decrease the number of posts left to scrape\n",
    "        posts_left = posts_left - posts_per_request\n",
    "        \n",
    "        # Convert the list of dictionaries created above to a pandas dataframe\n",
    "        new_df, last_post_time = posts_to_dataframe(data, verbose=verbose, posts_left=posts_left, total_posts=total_posts, name=subreddit_name)\n",
    "        \n",
    "        # concatenate the pandas dataframe created above to the dataframe of all posts scraped so far.\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    # Final save\n",
    "    posts_to_csv(df, posts_left, total_posts, subreddit_name, intermediate=False)\n",
    "    \n",
    "    print(f\"\\n>>>>>>>>>>>>>>>>>>>> Finished Scraping {total_posts} posts from subreddit {subreddit_name} <<<<<<<<<<<<<<<<<<<<\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2bb7203-f670-4d11-b46f-46e178c8e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# Helper function to scrape_multiple_subreddits.\n",
    "# Takes in a pandas dataframe with reddit post information and returns a dataframe that only contains\n",
    "# the few columns we are interested in keeping.\n",
    "# ================================================================================================================\n",
    "def clean_reddit_data(df, subreddit_name, total_posts, columns_to_keep=['subreddit', 'selftext', 'title', 'created_utc']):\n",
    "    \n",
    "    df = df.loc[:, columns_to_keep]\n",
    "    \n",
    "    df.to_csv(f\"./data/Final/Clean_{subreddit_name}_data_{total_posts}_posts.csv\", index=False)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf593b1-6af0-4055-ab13-849bb3d22098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# This function calls the scrape_reddit function in a loop for a given list of subreddit names\n",
    "#\n",
    "# Optional parameters for \n",
    "# ================================================================================================================\n",
    "def scrape_multiple_subreddits(subreddit_names = ['wallstreetbets', 'CryptoCurrency'], url='https://api.pushshift.io/reddit/search/submission', posts_per_request=100,\n",
    "                               total_posts_per_subreddit=2000, save_every=500):\n",
    "    \n",
    "    clean_dfs = []\n",
    "    full_dfs = []\n",
    "    \n",
    "    for reddit_name in subreddit_names: \n",
    "        \n",
    "        # This section was added after the fact to add flexibility when collecting additional posts.\n",
    "        # The if/else below can be easily edited as needed to allow data collection to begin after any desired timestamp, and to allow for\n",
    "        # different numbers of posts to be colleted from each subreddit.\n",
    "        if reddit_name == 'NA':\n",
    "            warm = True\n",
    "            before = '1610672400'\n",
    "            file = \"./data/Intermediate/blank_warmstart_wallstreetbets.csv\"\n",
    "            completed = 0\n",
    "        else:\n",
    "            warm = True\n",
    "            before = \"1510724913\"\n",
    "            file = \"./data/Intermediate/blank_warmstart_crypto.csv\"\n",
    "            completed = 0\n",
    "        \n",
    "        # Call scrape_reddit for each of the desired subredits.\n",
    "        df = scrape_reddit(subreddit_name = reddit_name, url='https://api.pushshift.io/reddit/search/submission', posts_per_request=posts_per_request,\n",
    "                           total_posts=total_posts_per_subreddit, warm=warm, before=before, start_file=file, posts_completed=completed, save_every=save_every)\n",
    "        \n",
    "        # Keep a list of the \"full\" dataframes for each subreddit that is scraped.\n",
    "        full_dfs.append(df)\n",
    "        \n",
    "        # Remove columns that are not going to be used.\n",
    "        df = clean_reddit_data(df, subreddit_name=reddit_name, total_posts=total_posts_per_subreddit)\n",
    "        \n",
    "        # Keep a list of the reduced size dataframes for each subreddit that is scraped.\n",
    "        clean_dfs.append(df)\n",
    "    \n",
    "    \n",
    "    print(\"======================================== Scrape complete! ========================================\")\n",
    "    print(f\"Successfully collected {total_posts_per_subreddit} from each of the following subreddits ---> {subreddit_names}\")\n",
    "    print(\"==========================================================================================\")\n",
    "    \n",
    "    \n",
    "    all_data = {'full_dfs' : full_dfs,\n",
    "                \"clean_dfs\" : clean_dfs}\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab11a688-9ff9-4d9f-97a6-8b3952752fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm start...0 posts have been scraped already, 125000 more to go\n",
      "\n",
      "==============================================================================\n",
      "Successfully obtained 100 of 125000 posts so far in subreddit wallstreetbets.\n",
      "First post in this batch created at UTC: 1610672395\n",
      "Last post in this batch created at UTC: 1610670451\n",
      "There are 124900 more posts to collect.\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "Successfully obtained 25100 of 125000 posts so far in subreddit wallstreetbets.\n",
      "First post in this batch created at UTC: 1608740238\n",
      "Last post in this batch created at UTC: 1608737264\n",
      "There are 99900 more posts to collect.\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "Successfully obtained 50100 of 125000 posts so far in subreddit wallstreetbets.\n",
      "First post in this batch created at UTC: 1606768021\n",
      "Last post in this batch created at UTC: 1606764832\n",
      "There are 74900 more posts to collect.\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "Successfully obtained 75100 of 125000 posts so far in subreddit wallstreetbets.\n",
      "First post in this batch created at UTC: 1604692089\n",
      "Last post in this batch created at UTC: 1604533851\n",
      "There are 49900 more posts to collect.\n",
      "==============================================================================\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "Successfully obtained 100100 of 125000 posts so far in subreddit wallstreetbets.\n",
      "First post in this batch created at UTC: 1600455936\n",
      "Last post in this batch created at UTC: 1600448603\n",
      "There are 24900 more posts to collect.\n",
      "==============================================================================\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>> Finished Scraping 125000 posts from subreddit wallstreetbets <<<<<<<<<<<<<<<<<<<<\n",
      "\n",
      "======================================== Scrape complete! ========================================\n",
      "Successfully collected 125000 from each of the following subreddits ---> ['wallstreetbets']\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "scrape_dataframes = scrape_multiple_subreddits(subreddit_names = ['wallstreetbets'], total_posts_per_subreddit=125_000, save_every=25000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
