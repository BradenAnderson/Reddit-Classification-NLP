{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oriXcDKPtv21"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, SCORERS, multilabel_confusion_matrix, make_scorer, roc_curve, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pQe45jezt1Wp",
    "outputId": "73fa1300-20ca-466c-df81-28f1d0618153"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>all_text_data</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1532973281</td>\n",
       "      <td>tron justin sun stop short reveal secret project</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550248670</td>\n",
       "      <td>reuters hsbc forex trading cost cut sharply bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1612224578</td>\n",
       "      <td>put expire 5th think clown brain read</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1543091767</td>\n",
       "      <td>toilet french start name trone concidence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1571098829</td>\n",
       "      <td>apollo 11 jpeg yes roomba ltolgt microsoft wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc                                      all_text_data  subreddit\n",
       "0   1532973281   tron justin sun stop short reveal secret project          1\n",
       "1   1550248670  reuters hsbc forex trading cost cut sharply bl...          1\n",
       "2   1612224578              put expire 5th think clown brain read          0\n",
       "3   1543091767          toilet french start name trone concidence          1\n",
       "4   1571098829  apollo 11 jpeg yes roomba ltolgt microsoft wor...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"./data/Processed/increment_train_size/5_to_20_words_preprocessed_TEST.csv\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hO89yeuot10G"
   },
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# Helper function used to convert a gridsearch output into a pandas dataframe with the columns formatted the way I like them. \n",
    "# ===========================================================================================================================\n",
    "def gs_to_clean_df(search_results, keep_split = False, keep_std = False, keep_time = False, keep_params = False, sort_by=None, make_RMSE=False):\n",
    "\n",
    "    gs_results_df = pd.DataFrame(search_results)\n",
    "\n",
    "    gs_result_columns = list(gs_results_df.columns)\n",
    "    throw_away_columns = []\n",
    "    columns_to_keep = []\n",
    "    columns_renamed = []\n",
    "    valid_metrics = []\n",
    "\n",
    "    for column_name in gs_result_columns: \n",
    "\n",
    "        if column_name.startswith('split'):\n",
    "            if keep_split == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif 'time' in column_name: \n",
    "            if keep_time == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif column_name.startswith('std'):\n",
    "            if keep_std == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif column_name == 'params':\n",
    "            if keep_params == True:\n",
    "                columns_to_keep.append(column_name)\n",
    "            else:\n",
    "                throw_away_columns.append(column_name)\n",
    "        else: \n",
    "            columns_to_keep.append(column_name)\n",
    "\n",
    "    gs_results_df.drop(labels=throw_away_columns, axis='columns', inplace=True)\n",
    "    renaming_dict = {}\n",
    "\n",
    "    for column_name in columns_to_keep: \n",
    "        name = \"\"\n",
    "\n",
    "        if column_name.startswith('param') and column_name != 'params': \n",
    "            name_components = column_name.split('__')\n",
    "\n",
    "            name_components = name_components[1:]\n",
    "\n",
    "            for component in name_components:\n",
    "                name = name + '_' + component \n",
    "                \n",
    "            name = name.lstrip('_')\n",
    "            \n",
    "        elif '_test' in column_name:\n",
    "            name = column_name.replace('_test', '')\n",
    "\n",
    "        renaming_dict[column_name] = name\n",
    "\n",
    "        if name.startswith('rank') or name.startswith('mean'):\n",
    "            valid_metrics.append(name)\n",
    "\n",
    "    gs_results_df.rename(columns=renaming_dict, inplace=True)\n",
    "\n",
    "    if sort_by in valid_metrics:\n",
    "        gs_results_df.sort_values(by=sort_by, inplace=True, ignore_index=True)\n",
    "\n",
    "    if make_RMSE:\n",
    "        gs_results_df['mean_RMSE'] = (abs(gs_results_df['mean_MSE'])) ** (1/2)\n",
    "\n",
    "    return gs_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KLjA5zGr6ZKS"
   },
   "outputs": [],
   "source": [
    "def print_test_score(test_df, model, params):\n",
    "    \n",
    "    X_test = test_df['all_text_data']\n",
    "    y_true = test_df['subreddit']\n",
    "    \n",
    "    print(\"==================================================================================================\")\n",
    "    print(f\"Model params:\\n {params}\\n\")\n",
    "    print(f\"Test set score: {model.score(X_test, y_true)}\")\n",
    "    print(\"==================================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DAt_xflBt5TL"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# Print the number of words in the shortest and longest post in the dataset.\n",
    "# ===============================================================================================\n",
    "def print_word_counts(df):\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    df['all_text_data'] = df['all_text_data'].astype(str)\n",
    "    \n",
    "    df['word_count'] = df['all_text_data'].apply(lambda text: len(text.split()))\n",
    "    \n",
    "    max_words = df['word_count'].max()\n",
    "    min_words = df['word_count'].min()\n",
    "    \n",
    "    print(f\"Maximum length post: {max_words} words\")\n",
    "    print(f\"Minimum length post: {min_words} words\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NBjttwfSt6dO"
   },
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# This function takes as input a dataframe containing reddit posts and returns the text data and\n",
    "# target split apart.\n",
    "#\n",
    "# This function also performs some final checks to make sure the data is correct prior to\n",
    "# attempting to build any models with it.\n",
    "# ===============================================================================================\n",
    "def data_check(train_df, pattern=r'(?u)\\b\\w+\\b', verify_regex=False):\n",
    "    \n",
    "    print(f\"Distribution:\\n {train_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "    \n",
    "    print(f\"Missing values: {train_df['all_text_data'].isna().sum()}\\n\")\n",
    "            \n",
    "    X = train_df.loc[:, 'all_text_data'].to_numpy()\n",
    "    y = train_df.loc[:, 'subreddit'].to_numpy().ravel() \n",
    "    \n",
    "    print(f\"Number of duplicates in training set: {train_df.duplicated().sum()}\\n\")\n",
    "    print(f\"Number of samples in training set: {len(train_df.index)}\\n\")\n",
    "    \n",
    "    print_word_counts(train_df)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaANFcakt8L6"
   },
   "source": [
    "## 5000 sample grid searches \n",
    "\n",
    "Data only includes posts 5 to 20 words in length.\n",
    "\n",
    "\n",
    "Only given 5000 samples of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEVGHC9Rt8id",
    "outputId": "3ec326a5-8c10-41ff-91d9-3cc702c40eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      " 0    0.5026\n",
      "1    0.4974\n",
      "Name: subreddit, dtype: float64\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "Number of duplicates in training set: 0\n",
      "\n",
      "Number of samples in training set: 5000\n",
      "\n",
      "Maximum length post: 20 words\n",
      "Minimum length post: 5 words\n"
     ]
    }
   ],
   "source": [
    "# Read in the training dataset that contains 5000 samples.\n",
    "train_df = pd.read_csv(\"./data/Processed/increment_train_size/train5000/train_5000.csv\")\n",
    "\n",
    "# Split into X and y. \n",
    "X, y = data_check(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DOiwPXzUuANF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"nb\", MultinomialNB())])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,1), (1,2)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'nb__alpha' : [0.1, 0.5, 1, 1.5], \n",
    "                   'nb__fit_prior' : [True, False]}]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=1)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/nb_5000_gs1.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "C4LkklU3utnV",
    "outputId": "fa1e60e4-28f0-4550-c5d5-9f2f67bf2444"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fit_prior</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range alpha fit_prior  mean_score  rank_score\n",
       "0     word         8761      (1, 2)   1.5      True      0.9084           1\n",
       "1     word         8761      (1, 1)     1     False      0.9082           2\n",
       "2     word         8761      (1, 1)     1      True      0.9080           3\n",
       "3     word         8761      (1, 2)   1.5     False      0.9078           4\n",
       "4     word         8761      (1, 1)   1.5     False      0.9070           5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/nb_5000_gs1.pkl'\n",
    "\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results = pickle.load(file)\n",
    "\n",
    "search_results = gs_results.cv_results_\n",
    "top_estimator = gs_results.best_estimator_\n",
    "top_parameters = gs_results.best_params_\n",
    "\n",
    "gs1_df = gs_to_clean_df(search_results, sort_by='rank_score')\n",
    "gs1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "al4wTJVhu8m-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"nb\", MultinomialNB())])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,2), (1,3)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'nb__alpha' : [0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0], \n",
    "                   'nb__fit_prior' : [True, False]}]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=1)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/nb_5000_gs2.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5w96bSg0vQIj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fit_prior</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.6</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.7</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.8</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range alpha fit_prior  mean_score  rank_score\n",
       "0     word         8761      (1, 2)   1.6      True      0.9088           1\n",
       "1     word         8761      (1, 2)   1.7      True      0.9086           2\n",
       "2     word         8761      (1, 2)   1.7     False      0.9086           2\n",
       "3     word         8761      (1, 2)   1.5      True      0.9084           4\n",
       "4     word         8761      (1, 2)   1.8      True      0.9084           5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/nb_5000_gs2.pkl'\n",
    "\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results2 = pickle.load(file)\n",
    "    \n",
    "search_results2 = gs_results2.cv_results_\n",
    "top_estimator2 = gs_results2.best_estimator_\n",
    "top_parameters2 = gs_results2.best_params_\n",
    "\n",
    "gs2_df = gs_to_clean_df(search_results2, sort_by='rank_score')\n",
    "gs2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GzHFn9fKvWlZ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"nb\", MultinomialNB())])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'nb__alpha' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0], \n",
    "                   'nb__fit_prior' : [False]}]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=1)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/nb_5000_noFitPrior_gs1.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "C0zhvF7GGQMl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fit_prior</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.9</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range alpha fit_prior  mean_score  rank_score\n",
       "0     word         8761      (1, 2)   1.7     False      0.9086           1\n",
       "1     word         8761      (1, 2)   1.6     False      0.9082           2\n",
       "2     word         8761      (1, 2)   1.9     False      0.9082           2\n",
       "3     word         8761      (1, 1)     1     False      0.9082           2\n",
       "4     word         8761      (1, 1)   1.1     False      0.9080           5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/nb_5000_noFitPrior_gs1.pkl'\n",
    "\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results3 = pickle.load(file)\n",
    "\n",
    "top_estimator3 = gs_results3.best_estimator_\n",
    "search_results3 = gs_results3.cv_results_\n",
    "top_parameters3 = gs_results3.best_params_\n",
    "\n",
    "gs3_df = gs_to_clean_df(search_results3, sort_by='rank_score')\n",
    "gs3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iC9l7wR369ok"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'nb__alpha': 1.5, 'nb__fit_prior': True}\n",
      "\n",
      "Test set score: 0.91264\n",
      "==================================================================================================\n",
      "\n",
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'nb__alpha': 1.6, 'nb__fit_prior': True}\n",
      "\n",
      "Test set score: 0.9128\n",
      "==================================================================================================\n",
      "\n",
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'nb__alpha': 1.7, 'nb__fit_prior': False}\n",
      "\n",
      "Test set score: 0.91288\n",
      "==================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters for each of the Gradient Boosted Decision Tree models. \n",
    "print_test_score(test_df, top_estimator, top_parameters)\n",
    "print_test_score(test_df, top_estimator2, top_parameters2)\n",
    "print_test_score(test_df, top_estimator3, top_parameters3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "05_Naive_Bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
