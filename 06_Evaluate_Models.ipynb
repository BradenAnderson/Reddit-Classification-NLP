{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ea0d15-aa5a-4e13-8b23-36dde7626868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, SCORERS, multilabel_confusion_matrix, make_scorer, roc_curve, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8123eb3-9310-436d-8cd7-b74cfd650e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>all_text_data</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1532973281</td>\n",
       "      <td>tron justin sun stop short reveal secret project</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550248670</td>\n",
       "      <td>reuters hsbc forex trading cost cut sharply bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1612224578</td>\n",
       "      <td>put expire 5th think clown brain read</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1543091767</td>\n",
       "      <td>toilet french start name trone concidence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1571098829</td>\n",
       "      <td>apollo 11 jpeg yes roomba ltolgt microsoft wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc                                      all_text_data  subreddit\n",
       "0   1532973281   tron justin sun stop short reveal secret project          1\n",
       "1   1550248670  reuters hsbc forex trading cost cut sharply bl...          1\n",
       "2   1612224578              put expire 5th think clown brain read          0\n",
       "3   1543091767          toilet french start name trone concidence          1\n",
       "4   1571098829  apollo 11 jpeg yes roomba ltolgt microsoft wor...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the test data used to evaluate models.\n",
    "test_df = pd.read_csv(\"./data/Processed/increment_train_size/5_to_20_words_preprocessed_TEST.csv\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ccb1e57-78fd-4a19-ac0d-2dbf5309a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# Helper function used to convert a gridsearch output into a pandas dataframe with the columns formatted the way I like them. \n",
    "# ===========================================================================================================================\n",
    "def gs_to_clean_df(search_results, keep_split = False, keep_std = False, keep_time = False, keep_params = False, sort_by=None, make_RMSE=False):\n",
    "\n",
    "    gs_results_df = pd.DataFrame(search_results)\n",
    "\n",
    "    gs_result_columns = list(gs_results_df.columns)\n",
    "    throw_away_columns = []\n",
    "    columns_to_keep = []\n",
    "    columns_renamed = []\n",
    "    valid_metrics = []\n",
    "\n",
    "    for column_name in gs_result_columns: \n",
    "\n",
    "        if column_name.startswith('split'):\n",
    "            if keep_split == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif 'time' in column_name: \n",
    "            if keep_time == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif column_name.startswith('std'):\n",
    "            if keep_std == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif column_name == 'params':\n",
    "            if keep_params == True:\n",
    "                columns_to_keep.append(column_name)\n",
    "            else:\n",
    "                throw_away_columns.append(column_name)\n",
    "        else: \n",
    "            columns_to_keep.append(column_name)\n",
    "\n",
    "    gs_results_df.drop(labels=throw_away_columns, axis='columns', inplace=True)\n",
    "    renaming_dict = {}\n",
    "\n",
    "    for column_name in columns_to_keep: \n",
    "        name = \"\"\n",
    "\n",
    "        if column_name.startswith('param') and column_name != 'params': \n",
    "            name_components = column_name.split('__')\n",
    "\n",
    "            name_components = name_components[1:]\n",
    "\n",
    "            for component in name_components:\n",
    "                name = name + '_' + component \n",
    "                \n",
    "            name = name.lstrip('_')\n",
    "            \n",
    "        elif '_test' in column_name:\n",
    "            name = column_name.replace('_test', '')\n",
    "\n",
    "        renaming_dict[column_name] = name\n",
    "\n",
    "        if name.startswith('rank') or name.startswith('mean'):\n",
    "            valid_metrics.append(name)\n",
    "\n",
    "    gs_results_df.rename(columns=renaming_dict, inplace=True)\n",
    "\n",
    "    if sort_by in valid_metrics:\n",
    "        gs_results_df.sort_values(by=sort_by, inplace=True, ignore_index=True)\n",
    "\n",
    "    if make_RMSE:\n",
    "        gs_results_df['mean_RMSE'] = (abs(gs_results_df['mean_MSE'])) ** (1/2)\n",
    "\n",
    "    return gs_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdca0c7-dd8f-4fc4-962d-705321f657e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# Print the number of words in the shortest and longest post in the dataset.\n",
    "# ===============================================================================================\n",
    "def print_word_counts(df):\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    df['all_text_data'] = df['all_text_data'].astype(str)\n",
    "    \n",
    "    df['word_count'] = df['all_text_data'].apply(lambda text: len(text.split()))\n",
    "    \n",
    "    max_words = df['word_count'].max()\n",
    "    min_words = df['word_count'].min()\n",
    "    \n",
    "    print(f\"Maximum length post: {max_words} words\")\n",
    "    print(f\"Minimum length post: {min_words} words\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e032a676-7583-4bfe-919b-973d1922db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# This function takes as input a dataframe with reddit posts and returns the text data and\n",
    "# target as X and y.\n",
    "#\n",
    "# This function also performs some final checks to make sure the data is correct prior to\n",
    "# attempting to build any models with it.\n",
    "# ===============================================================================================\n",
    "def data_check(train_df):\n",
    "    \n",
    "    print(f\"Distribution:\\n {train_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "    \n",
    "    print(f\"Missing values: {train_df['all_text_data'].isna().sum()}\\n\")\n",
    "            \n",
    "    X = train_df.loc[:, 'all_text_data'].to_numpy()\n",
    "    y = train_df.loc[:, 'subreddit'].to_numpy().ravel() \n",
    "    \n",
    "    print(f\"Number of duplicates in training set: {train_df.duplicated().sum()}\\n\")\n",
    "    print(f\"Number of samples in training set: {len(train_df.index)}\\n\")\n",
    "    \n",
    "    print_word_counts(train_df)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e05def-6b0f-49bb-9140-ccb5e03b6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs1.pkl'\n",
    "\n",
    "# Read in the model file for the best \"small\" gradient boosted decision tree model.\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results_little_trees = pickle.load(file)\n",
    "\n",
    "# Save the gridsearch results\n",
    "search_results_little_trees = gs_results_little_trees.cv_results_\n",
    "top_estimator_little_trees = gs_results_little_trees.best_estimator_\n",
    "top_parameters_little_trees = gs_results_little_trees.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a453e8-a897-40aa-9245-70f30edaf8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range colsample_bytree learning_rate max_depth  \\\n",
       "0     word         8761      (1, 2)              1.0          0.05         9   \n",
       "1     word         8761      (1, 2)              1.0          0.05         9   \n",
       "2     word         8761      (1, 2)              1.0          0.05         5   \n",
       "3     word         8761      (1, 2)              1.0          0.05         5   \n",
       "4     word         8761      (1, 2)              0.8          0.05         5   \n",
       "\n",
       "  n_estimators subsample  mean_score  rank_score  \n",
       "0          300       1.0      0.8822           1  \n",
       "1          300       0.8      0.8822           1  \n",
       "2          500       1.0      0.8820           3  \n",
       "3          500       0.8      0.8820           3  \n",
       "4          500       1.0      0.8814           5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the gridsearch results\n",
    "gs_lt_df = gs_to_clean_df(search_results_little_trees, sort_by='rank_score')\n",
    "gs_lt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f77320-e6de-4b85-8400-e902fb86dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs5.pkl'\n",
    "\n",
    "# Read in the model file for the best \"large\" gradient boosted decision tree model.\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results_big_trees = pickle.load(file)\n",
    "\n",
    "# Save the gridsearch results\n",
    "search_results_big_trees = gs_results_big_trees.cv_results_\n",
    "top_estimator_big_trees = gs_results_big_trees.best_estimator_\n",
    "top_score_big_trees = gs_results_big_trees.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05f3bb0-c3aa-4c5d-94a9-2dbbd8b71913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13</td>\n",
       "      <td>1200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range colsample_bytree learning_rate max_depth  \\\n",
       "0     word         8761      (1, 2)              0.9          0.01        13   \n",
       "1     word         8761      (1, 2)              0.9          0.01        13   \n",
       "2     word         8761      (1, 2)              0.9          0.01        13   \n",
       "3     word         8761      (1, 2)              0.9          0.01        14   \n",
       "4     word         8761      (1, 2)              0.9          0.01        14   \n",
       "\n",
       "  n_estimators subsample  mean_score  rank_score  \n",
       "0         1200       0.8      0.8834           1  \n",
       "1         1200       0.9      0.8834           1  \n",
       "2         1200       1.0      0.8834           1  \n",
       "3         1150       1.0      0.8832           4  \n",
       "4         1150       0.9      0.8832           4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the gridsearch results\n",
    "gs_bt_df = gs_to_clean_df(search_results_big_trees, sort_by='rank_score')\n",
    "gs_bt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d437827-457a-4f5c-bc65-0ed3a9d4602a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fit_prior</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.9</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1.1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range alpha fit_prior  mean_score  rank_score\n",
       "0     word         8761      (1, 2)   1.7     False      0.9086           1\n",
       "1     word         8761      (1, 2)   1.6     False      0.9082           2\n",
       "2     word         8761      (1, 2)   1.9     False      0.9082           2\n",
       "3     word         8761      (1, 1)     1     False      0.9082           2\n",
       "4     word         8761      (1, 1)   1.1     False      0.9080           5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Naive Bayes Model that did not use the hyperparameter fit_prior=False\n",
    "PATH_No_FitPrior = './data/Processed/increment_train_size/pickle/nb_5000_noFitPrior_gs1.pkl'\n",
    "\n",
    "# Read in the trained model file at the path listed above.\n",
    "with open(PATH_No_FitPrior, 'rb') as file:\n",
    "    gs_results_nb_np = pickle.load(file)\n",
    "    top_estimator_nb_np = gs_results_nb_np.best_estimator_\n",
    "\n",
    "# Save the gridsearch results\n",
    "search_results_nb_np = gs_results_nb_np.cv_results_\n",
    "\n",
    "# Display the gridsearch results\n",
    "gs_nb_np_df = gs_to_clean_df(search_results_nb_np, sort_by='rank_score')\n",
    "gs_nb_np_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed744cf5-5af9-4717-9336-d71419ba779e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>alpha</th>\n",
       "      <th>fit_prior</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9080</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1.5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range alpha fit_prior  mean_score  rank_score\n",
       "0     word         8761      (1, 2)   1.5      True      0.9084           1\n",
       "1     word         8761      (1, 1)     1     False      0.9082           2\n",
       "2     word         8761      (1, 1)     1      True      0.9080           3\n",
       "3     word         8761      (1, 2)   1.5     False      0.9078           4\n",
       "4     word         8761      (1, 1)   1.5     False      0.9070           5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Naive Bayes Model that did use the hyperparameter fit_prior=True\n",
    "PATH_FitPrior = './data/Processed/increment_train_size/pickle/nb_5000_gs1.pkl'\n",
    "\n",
    "# Read in the trained model file at the path listed above.\n",
    "with open(PATH_FitPrior, 'rb') as file:\n",
    "    gs_results_nb_wp = pickle.load(file)\n",
    "    top_estimator_nb_wp = gs_results_nb_wp.best_estimator_\n",
    "    \n",
    "# Save the gridsearch results\n",
    "search_results_nb_wp = gs_results_nb_wp.cv_results_\n",
    "\n",
    "# Display the gridsearch results\n",
    "gs_nb_wp_df = gs_to_clean_df(search_results_nb_wp, sort_by='rank_score')\n",
    "gs_nb_wp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "832cead1-d283-4492-8df3-6243e69241f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================\n",
    "# This function the following inputs:\n",
    "#\n",
    "# 1) A trained model\n",
    "# 2) A dataframe containing a test dataset\n",
    "# 3) The size of the training set used, which is only needed to record the value in the results dataframe\n",
    "# 4) A results dataframe that tracks this models accuracy on each training set size. This dataframe\n",
    "#    will be updated to include its performance with this training set size and will be returned. \n",
    "# =========================================================================================================\n",
    "def update_score_df(model, test_df, train_size, df):\n",
    "    \n",
    "    # This models parameters, which will be recorded in the results dataframe.\n",
    "    params = model.named_steps\n",
    "    \n",
    "    # Split the test data into features and target\n",
    "    X_test = test_df['all_text_data']\n",
    "    y_true = test_df['subreddit']\n",
    "    \n",
    "    # Score the trained model on the test data.\n",
    "    score = model.score(X_test, y_true)\n",
    "    \n",
    "    # Create a new dataframe containing the models results from the above test.\n",
    "    new_df = pd.DataFrame({'Train_Size' : [train_size], 'Test_Accuracy' : [score], 'Model_Params' : [params]})\n",
    "    \n",
    "    # Concatentate this result to the dataframe that has the models results from all training set sizes that have been evaluated. \n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0996c5-0ee7-49d9-b9be-368176793a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================\n",
    "# This function takes the following inputs:\n",
    "#\n",
    "# model: A tuple containing a \"best estimator\" found by gridsearch and the name of the model type\n",
    "# formatted as (model, name).\n",
    "#\n",
    "# train_set_sizes: A list of training set sizes that we want to use to train this model and evaluate its \n",
    "# performance.\n",
    "#\n",
    "# score_df: A dataframe we will use to keep track of how well the model performs after each round\n",
    "#           of training and testing.\n",
    "#\n",
    "# test_df: A dataframe containing the test dataset that the models performance will be evaluated on. \n",
    "#\n",
    "# Outputs:\n",
    "# \n",
    "# score_df: The dataframe of models performance after being trained on sets of each size specified in\n",
    "# train_set_sizes.\n",
    "#\n",
    "# =========================================================================================================\n",
    "def evaluate_model(model, train_set_sizes, score_df, test_df, save_every=None): \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # Unpack the \"model\" tuple into the actual model and the name of this model.\n",
    "    model, name = model\n",
    "    \n",
    "    try: \n",
    "        model_dir = f\"./data/Processed/increment_train_size/model_test_scores/{name}/\"\n",
    "        os.mkdir(model_dir)\n",
    "    except:\n",
    "        print(f\"Directory {model_dir} already exists!\")\n",
    "        print(\"Proceeding to the model training and evaluation loop...\\n\")\n",
    "    \n",
    "    # For each training set size we want to evaluate the model for. \n",
    "    for train_size in train_set_sizes:\n",
    "        \n",
    "        # Read in the training set of the appropriate size.\n",
    "        train_df = pd.read_csv(f\"./data/Processed/increment_train_size/train{train_size}/train_{train_size}.csv\")\n",
    "        \n",
    "        # Split into features and target.\n",
    "        X_train, y_train = data_check(train_df)\n",
    "        \n",
    "        # Fit the model.\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Test the model and update the results dataframe with its accuracy. \n",
    "        score_df = update_score_df(model=model, test_df=test_df, train_size=train_size, df=score_df)\n",
    "        \n",
    "        # If we have performed save_every number of evaluations since the last checkpoint save. \n",
    "        if (save_every is not None) and (count % save_every == 0):\n",
    "            print(f\"Updating: test_{name}_trainSize_{train_size}.csv\\n\")\n",
    "            score_df.to_csv(f\"./data/Processed/increment_train_size/model_test_scores/{name}/test_{name}_trainSize_{train_size}.csv\", index=False)\n",
    "        \n",
    "        # increment the number of evaluations that have been performed.\n",
    "        count+=1\n",
    "        \n",
    "    score_df.to_csv(f\"./data/Processed/increment_train_size/model_test_scores/{name}/test_{name}_trainSize_{train_size}_FINAL.csv\", index=False)\n",
    "    \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee658790-a5ec-4f04-b0b0-6ce887091d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================\n",
    "# This function calls the evaluate_model function for a list of models that is provided to the models parameter.\n",
    "# =========================================================================================================\n",
    "def evaluate_models(models, train_set_sizes, score_dfs, test_df, save_every=10):\n",
    "    \n",
    "    updated_score_dfs = []\n",
    "    \n",
    "    for model, score_df in zip(models, score_dfs):\n",
    "        \n",
    "        df = evaluate_model(model=model, train_set_sizes=train_set_sizes, score_df=score_df, test_df = test_df, save_every=save_every)\n",
    "        \n",
    "        updated_score_dfs.append(df)\n",
    "    \n",
    "    return updated_score_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75ba39c5-72b7-44a6-8037-e58b46cb41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes to collect information on each models performance after being trained on each different size training set.\n",
    "gs_lt_test_df = pd.DataFrame({'Train_Size' : [], 'Test_Accuracy' : [], 'Model_Params' : []})\n",
    "gs_bt_test_df = pd.DataFrame({'Train_Size' : [], 'Test_Accuracy' : [], 'Model_Params' : []})\n",
    "nb_wp_test_df = pd.DataFrame({'Train_Size' : [], 'Test_Accuracy' : [], 'Model_Params' : []})\n",
    "nb_np_test_df = pd.DataFrame({'Train_Size' : [], 'Test_Accuracy' : [], 'Model_Params' : []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d23cdf9-14a9-49a1-a9a3-0dca69359118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of training set sizes each model should be trained with.\n",
    "\n",
    "# Note: Make sure all of these datasets have been created by the increment_train_set_size notebook before trying to run this!! \n",
    "train_sizes = [100 * n for n in range(1, 21)] + [250 * n for n in range(9, 20)] + [5000 * n for n in range(1, 101)]\n",
    "\n",
    "# Evaluate the performance of each model in the models list, using the test data in test_df, after it has been trained using training sets of \n",
    "# each size listed in train_set_sizes.\n",
    "score_dfs = evaluate_models(models=[(top_estimator_little_trees, \"little_trees\"),\n",
    "                                    (top_estimator_big_trees, \"big_trees\"),\n",
    "                                    (top_estimator_nb_np, \"naive_bayes_NO_fit_prior\"),\n",
    "                                    (top_estimator_nb_wp, \"naive_bayes_WITH_fit_prior\")],\n",
    "                            train_set_sizes = train_sizes,\n",
    "                            score_dfs=[gs_lt_test_df,\n",
    "                                       gs_bt_test_df,\n",
    "                                       nb_wp_test_df,\n",
    "                                       nb_np_test_df],\n",
    "                            test_df=test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
