{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d593d2d1-6993-4696-adac-24f32749b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (1.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (8.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-lg==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from en-core-web-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.59.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (20.9)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (5.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\braden\\anaconda3\\envs\\dsi\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Run this if spacy has not been installed on your machine yet. \n",
    "#! pip install spacy\n",
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ecd4155-aa47-4fe8-bb78-f1d8d5777ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "import en_core_web_lg\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "# Global variables to track how long spaCy is taking to\n",
    "# perform lemmatization. \n",
    "global lem_counter\n",
    "lem_counter = 0\n",
    "\n",
    "global notebook_clock\n",
    "notebook_clock = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8e221c-73da-4e88-88c0-11a8933578dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>all_text_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All Nodes Ethereum 2.0 Services</td>\n",
       "      <td>1624735574</td>\n",
       "      <td>all nodes ethereum 20 services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Is the Largest Difficulty Adjustment In Bitcoi...</td>\n",
       "      <td>1624735572</td>\n",
       "      <td>is the largest difficulty adjustment in bitcoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binance leaving Ontario: Binance will handle E...</td>\n",
       "      <td>1624735503</td>\n",
       "      <td>binance leaving ontario binance will handle et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>I own some purely because of the doge effect. ...</td>\n",
       "      <td>What are your thoughts on shib?</td>\n",
       "      <td>1624735442</td>\n",
       "      <td>what are your thoughts on shib i own some pure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miami Beach's most expensive penthouse just so...</td>\n",
       "      <td>1624735282</td>\n",
       "      <td>miami beachs most expensive penthouse just sol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                           selftext  \\\n",
       "0  CryptoCurrency                                                NaN   \n",
       "1  CryptoCurrency                                                NaN   \n",
       "2  CryptoCurrency                                                NaN   \n",
       "3  CryptoCurrency  I own some purely because of the doge effect. ...   \n",
       "4  CryptoCurrency                                                NaN   \n",
       "\n",
       "                                               title  created_utc  \\\n",
       "0                    All Nodes Ethereum 2.0 Services   1624735574   \n",
       "1  Is the Largest Difficulty Adjustment In Bitcoi...   1624735572   \n",
       "2  Binance leaving Ontario: Binance will handle E...   1624735503   \n",
       "3                    What are your thoughts on shib?   1624735442   \n",
       "4  Miami Beach's most expensive penthouse just so...   1624735282   \n",
       "\n",
       "                                       all_text_data  \n",
       "0                     all nodes ethereum 20 services  \n",
       "1  is the largest difficulty adjustment in bitcoi...  \n",
       "2  binance leaving ontario binance will handle et...  \n",
       "3  what are your thoughts on shib i own some pure...  \n",
       "4  miami beachs most expensive penthouse just sol...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data that has been preprocessed by the 01_Data_Cleaning.ipynb notebook.\n",
    "reddit_df = pd.read_csv(\"./data/Processed/wsb_crypto_preprocessed_2073132.csv\")\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d65ce853-2cab-4fa4-9477-156de274ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of words that we want to add the default stop word list spaCy uses.\n",
    "words = ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'I', 'j', 'k', 'l', 'm', 'M', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'es', 'ing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1999b68-39e6-489c-b970-07a228be744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# This function uses the spacy model to perform lemmatization on each word in a single post.\n",
    "# ================================================================================================================\n",
    "def lemmatize_text(text, spaCy_nlp, verbose):\n",
    "    \n",
    "    # Note: lemmatization on large datasets can be quite slow. The if statement below is used to track\n",
    "    # progress by printing the total number of posts that have been lemmatized as well as the amount of \n",
    "    # time it has taken after every 10k posts processed.\n",
    "    global lem_counter\n",
    "    lem_counter = lem_counter + 1\n",
    "    if lem_counter % 10000 == 0:\n",
    "        print(f\"We successfully lemmatized {lem_counter} posts...\")\n",
    "        print(f\"The notebook clock has been running {time.time() - notebook_clock} seconds...\\n\\n\")\n",
    "      \n",
    "    # Turn the post into a spaCy doc object.\n",
    "    # Ensure the text is string type before we do.\n",
    "    doc = spaCy_nlp(str(text))\n",
    "    \n",
    "    # Use spacy to get the lemmas of each word. Discard any stopwords. \n",
    "    # Also make sure spaCy doesn't try to give us anything that is upper case. \n",
    "    lemmas = [token.lemma_.lower() for token in doc if (token.is_stop == False)]\n",
    "    \n",
    "    # Join the lemmatized text back together to form a single string\n",
    "    lemmatized_text = \" \".join(lemmas).strip()\n",
    "    \n",
    "    # This is for verifying the implementation only. Never recommended to use unless you suspect\n",
    "    # something is not working properly. This will print the original post and the lemmatized version.\n",
    "    if verbose and (lemmatized_text != text):\n",
    "        print(\"\\n===========================================================================\")\n",
    "        print(f\"Original: {text}\\n\")\n",
    "        print(f\"Lemmatized: {lemmatized_text}\\n\")\n",
    "        print(\"===========================================================================\\n\")\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d55f32-8691-43df-a797-1b67d3ab57f6",
   "metadata": {},
   "source": [
    "# Functions to build a numeric dataset\n",
    "\n",
    "The four functions listed below add a capability to the preprocess function to have a numeric version of the dataset created automatically directly after lemmatization and stop word removal is performed. Creating the numeric dataset in this way is disabled by default because this is **not** the preferred method for creating such datasets. The best option for creating numeric datasets is to use the keras TextVectorization layer (shown in later notebooks) https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/. The numeric dataset capability here can serve as a backup if for whatever reason the keras textvectorization function is not available.\n",
    "\n",
    "build_word_frequency_dict\n",
    "\n",
    "make_frequency_based_word_map\n",
    "\n",
    "replace_words_with_numbers\n",
    "\n",
    "transform_texts_to_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f12c807-47fc-4d4d-b046-2f9a2e934f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_frequency_dict(df, time_stats):\n",
    "    \n",
    "    print(\"Starting to build all_text list.\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Convert the texts for all posts into one giant string\n",
    "    all_text = df.loc[:, 'all_text_data'].str.cat(sep=' ')\n",
    "    \n",
    "    if time_stats:\n",
    "        print(\"\\n=================================================\")\n",
    "        print(f\"Finished building all text list. That took {time.time()-start}\")\n",
    "        print(\"Going to split all_text into a list of words.\")\n",
    "        print(\"=================================================\\n\")\n",
    "    \n",
    "    # Split the giant string into one giant list\n",
    "    word_list = all_text.split()\n",
    "    \n",
    "    if time_stats:\n",
    "        print(\"\\n=================================================\")\n",
    "        print(\"Finished building word list... going to start building the freq_dict.\")\n",
    "        print(f\"The length of the word list is {len(word_list)}\")\n",
    "        print(f\"Total time in this function is now {time.time() - start}\")\n",
    "        print(\"=================================================\\n\")\n",
    "    \n",
    "    # Create a dictionary that maps each unique word to the number of times in shows up\n",
    "    # Across all reddit posts.    \n",
    "    freq_dict = {}\n",
    "    for word in word_list:\n",
    "        if word not in freq_dict.keys():\n",
    "            freq_dict[word] = 1\n",
    "        else:\n",
    "            freq_dict[word] = freq_dict[word] + 1\n",
    "        \n",
    "    if time_stats:\n",
    "        print(\"\\n=================================================\")\n",
    "        print(\"Finished building freq_dict... saving to csv and exiting.\")\n",
    "        print(f\"Total time in this function is now {time.time() - start}\")\n",
    "        print(\"=================================================\\n\")\n",
    "    \n",
    "    # Save the frequency count data to a .csv as a convenience, in case there is any need to review it later.\n",
    "    words = list(freq_dict.keys())\n",
    "    values = list(freq_dict.values())\n",
    "    freq_df = pd.DataFrame({'word' : words, 'frequency' : values})\n",
    "    freq_df.to_csv(\"./support_data/nice_to_have/word_frequencies.csv\", index=False)\n",
    "    \n",
    "    return freq_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc049335-fc3d-43c2-a49b-2b2ab4f3fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frequency_based_word_map(frequency_dict, time_stats):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Use the frequency dict to create a list of tuples sorted by frequency. \n",
    "    # The most frequent word will be the first item in the list\n",
    "    sorted_frequencies = sorted(list(frequency_dict.items()), key = lambda sublist : sublist[1], reverse = True)\n",
    "    \n",
    "    if time_stats:\n",
    "        print(\"\\n=========================================================\")\n",
    "        print(\"Turning freq_dict into a list of tuples, sorted by frequency\")\n",
    "        print(f\"Time to perform sorting: {time.time() - start}\")\n",
    "        print(\"Next, using the sorted tuple list to create a list of words only\")\n",
    "        print(\"=========================================================\\n\")\n",
    "    \n",
    "    # Create a list of just the words, but they will still be in the order most frequent ---> least frequent\n",
    "    words = [word for (word, num) in sorted_frequencies]\n",
    "    \n",
    "    if time_stats:\n",
    "        print(\"\\n=========================================================\")\n",
    "        print(\"Finished creating the word list\")\n",
    "        print(f\"Total time in this function is now: {time.time() - start}\")\n",
    "        print(\"Next, mapping each unique word to a unique number...\")\n",
    "        print(\"=========================================================\\n\")\n",
    "    \n",
    "    # Give each word a unique number. The most common word gets the smallest number.\n",
    "    # We start at 2 because 0 and 1 are going to be reserved for unknown and out of vocab words respectively.\n",
    "    unique_word_map = {word:(index+2) for (index,word) in enumerate(words)}\n",
    "    \n",
    "    # 'Mask token' (i.e. not a word), used for padding sequencies.\n",
    "    unique_word_map['mask'] = 0\n",
    "    \n",
    "    # 'Unknown' or 'Out of Vocab' token to map to 1\n",
    "    unique_word_map['<unk>'] = 1\n",
    "    \n",
    "    if time_stats:\n",
    "        print(\"\\n=========================================================\")\n",
    "        print(\"Saving and exiting the make_frequency_based_word_map function...\")\n",
    "        print(f\"total time in this function: {time.time() - start}\")\n",
    "        print(\"=========================================================\\n\")\n",
    "    \n",
    "    # Save the word ---> unique value map to a .csv in case it needs to be referenced later.\n",
    "    words = list(unique_word_map.keys())\n",
    "    unique_numbers = list(unique_word_map.values())\n",
    "    unique_map_df = pd.DataFrame({'word' : words, 'unique_value' : unique_numbers})\n",
    "    unique_map_df.to_csv(\"./support_data/nice_to_have/map_words_to_unique_numbers.csv\", index=False)\n",
    "    \n",
    "    return unique_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14908cb9-1496-4450-8d5a-9348c1cebaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_with_numbers(text, word_to_num_map):\n",
    "    \n",
    "    numbers = \" \".join([str(word_to_num_map[word]) for word in str(text).split()])\n",
    "    \n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770cd80a-74cc-4af1-992e-5a3418ab98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the same thing as the second half of the preprocess function. Left here in case its ever need separately. \n",
    "def transform_texts_to_numeric(df):\n",
    "    \n",
    "    num_samples = len(df.index)\n",
    "    \n",
    "    word_frequencies = build_word_frequency_dict(df, time_stats=True)\n",
    "    \n",
    "    word_to_unique_num_map = make_frequency_based_word_map(word_frequencies, time_stats=True)\n",
    "    \n",
    "    df['all_text_data_numeric'] = df['all_text_data'].apply(lambda text: replace_words_with_numbers(text, word_to_unique_num_map))\n",
    "    \n",
    "    df.to_csv(f\"./data/Processed/Processed_Numeric_Lemmatized_{num_samples}_ALL_DATA.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3ec13b-4c06-48a1-84c5-98694a562a0a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceff5b1a-6451-42a1-bf5d-c9a830a1507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates spaCys list of default stop words to include the additional words we\n",
    "# defined at the start of this notebook.\n",
    "def update_stop_words(nlp, new_stop=words):\n",
    "    \n",
    "    for word in new_stop:\n",
    "        nlp.vocab[word].is_stop = True\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b335dea0-84d6-46e9-9cce-d59aaf91465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "#\n",
    "# ================================================================================================================\n",
    "def preprocess(df, additional_stopwords=words, verbose=False, time_stats=True, build_numeric_dataset=False):\n",
    "    \n",
    "    if time_stats:\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Used for file names\n",
    "    num_samples = len(df.index)\n",
    "    \n",
    "    # Load the spaCy model.\n",
    "    nlp = en_core_web_lg.load()\n",
    "    \n",
    "    if time_stats:\n",
    "        print(\"\\n========================================================================\")\n",
    "        print(f\"Finished loading the spaCy model... About to update stop words.\")\n",
    "        print(f\"preprocess function has been running for: {time.time() - start_time}\")\n",
    "        print(\"========================================================================\\n\")\n",
    "    \n",
    "    # Update spaCys default stopwords to include some additional items.\n",
    "    nlp = update_stop_words(nlp, new_stop=additional_stopwords)\n",
    "\n",
    "    if time_stats:\n",
    "        print(\"\\n========================================================================\")\n",
    "        print(f\"Finished updating stop words... about to lemmatize.\")\n",
    "        print(f\"preprocess function has been running for: {time.time() - start_time}\")\n",
    "        print(\"========================================================================\\n\")    \n",
    "       \n",
    "    # Perform lemmatization on every word in every post. \n",
    "    df['all_text_data'] = df['all_text_data'].apply(lambda text : lemmatize_text(text, nlp, verbose=verbose))\n",
    "    \n",
    "    if time_stats:\n",
    "        print(\"\\n========================================================================\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!! LEMMATIZATION COMPLETE !!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"preprocess function has been running for: {time.time() - start_time}\")\n",
    "        print(\"========================================================================\\n\")\n",
    "    \n",
    "    # Save the lemmatized file to .csv\n",
    "    df.to_csv(f\"./data/Processed/NOTREAL_Processed_Through_Lemmatization_{num_samples}_ALL_DATA.csv\", index=False)\n",
    "    \n",
    "    if build_numeric_dataset:\n",
    "    \n",
    "        if time_stats:\n",
    "            print(\"\\n========================================================================\")\n",
    "            print(f\"Saved lem file to .csv... about to build word_freq dict.\")\n",
    "            print(f\"preprocess function has been running for: {time.time() - start_time}\")\n",
    "            print(\"========================================================================\\n\")\n",
    "\n",
    "        # Build a dictionary that maps each word to the number of times it occurs across all posts.\n",
    "        word_frequencies = build_word_frequency_dict(df, time_stats=time_stats)\n",
    "\n",
    "        if time_stats:\n",
    "            print(\"\\n========================================================================\")\n",
    "            print(f\"Finished building word_freq dict... now building word-->unique_num map.\")\n",
    "            print(f\"preprocess function has been running for: {time.time() - start_time}\")\n",
    "            print(\"========================================================================\\n\")\n",
    "\n",
    "        # Build a dictionary that maps each unique word to a unique number, based on its frequency\n",
    "        # More common words get smaller numbers. No words get 0 or 1 because they are reserved.\n",
    "        # So the most common word maps to 2.\n",
    "        word_to_unique_num_map = make_frequency_based_word_map(word_frequencies, time_stats=time_stats)\n",
    "\n",
    "        if time_stats:\n",
    "            print(\"\\n========================================================================\")\n",
    "            print(f\"Finished building word-->unique_num map... about to use the map\")\n",
    "            print(\"On the lemmatized posts to create a numeric version of the data...\")\n",
    "            print(f\"preprocess function has been running for: {time.time() - start_time}\")\n",
    "            print(\"========================================================================\\n\")\n",
    "\n",
    "        # Create a copy of df, which we will then use to convert the text data to numbers with the \n",
    "        # word_to_unique_num_map\n",
    "        number_df = df.copy(deep=True)\n",
    "\n",
    "        # Create a new column where all of the words in the posts have been converted to unique numbers\n",
    "        number_df['all_text_data_numeric'] = number_df['all_text_data'].apply(lambda text: replace_words_with_numbers(text, word_to_unique_num_map))\n",
    "\n",
    "        if time_stats:\n",
    "            print(\"\\n========================================================================\")\n",
    "            print(f\"Finished building numeric data! Preprocessing is now complete!\")\n",
    "            print(f\"Preprocessing function took a total of: {time.time() - start_time} seconds.\")\n",
    "            print(\"========================================================================\\n\")\n",
    "\n",
    "        # Save the numeric data to .csv\n",
    "        number_df.to_csv(f\"./data/Processed/Processed_Numeric_Lemmatized_{num_samples}_ALL_DATA.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8cecf-22e3-489b-a309-4c4bf56c1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform lemmatization and stop word removal on the dataframe read in at the start of this notebook.\n",
    "reddit_df = preprocess(reddit_df, time_stats=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
