{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMx6NVZvHpIv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BFYoq7524hN"
      },
      "source": [
        "# Uncomment and run if fasttext is not already installed.\n",
        "\n",
        "# ! git clone https://github.com/facebookresearch/fastText.git\n",
        "\n",
        "# ! pip install /content/fastText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzmK1HK-HCpP"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "import fasttext\n",
        "from fasttext.FastText import load_model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.data.experimental import save, load\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping\n",
        "from tensorflow.keras.metrics import Precision, Recall, SensitivityAtSpecificity, SpecificityAtSensitivity, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, PrecisionAtRecall\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCOuufNn2ztP"
      },
      "source": [
        "# Location of a text file containing the text from all posts in the ALL_POTENTIAL_TRAIN dataset. Used to train the fasttext model.\n",
        "reddit_text_path = r'/content/drive/MyDrive/Programming/Colab Notebooks/General_Assembly/Project_3_NLP/data/LSTM/fastText/reddit_posts.txt'\n",
        "\n",
        "# Location of the trained fasttext word vector model\n",
        "word_vector_file = r'/content/drive/MyDrive/Programming/Colab Notebooks/General_Assembly/Project_3_NLP/data/LSTM/fastText/fastText_vectors.bin'\n",
        "\n",
        "# All potential training data, used to train the fasttext model.\n",
        "reddit_dataframe_path = \"/content/drive/MyDrive/Programming/Colab Notebooks/General_Assembly/Project_3_NLP/data/iterating_data/5_to_20_words_preprocessed_ALL_POTENTIAL_TRAIN.csv\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_DX8YuMCMK9"
      },
      "source": [
        "# Creating a fasttext word vector model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW3ZSI1FCnRu"
      },
      "source": [
        "# ==========================================================================\n",
        "# This function is used to perform a quick check that the fasttext word\n",
        "# vectors were created correctly. The function trys to load the trained\n",
        "# fasttext model and perform a basic operation using the word vectors.\n",
        "# ==========================================================================\n",
        "def check_ft_model(wv_file):\n",
        "\n",
        "    ft_model = fasttext.FastText.load_model(wv_file)\n",
        "\n",
        "    print(\"verifying fastText model loads correctly by printing the nearest neighbors to rocket.\")\n",
        "    print(\"=======================================================\")\n",
        "    print(f\"Nearest Neighbors to rocket:\\n {list(ft_model.get_nearest_neighbors('rocket'))}\")\n",
        "    print(\"=======================================================\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TunTYeGlBDgK"
      },
      "source": [
        "# ==========================================================================\n",
        "# This function reads in the .csv containing all of the reddit posts and\n",
        "# uses the text in those posts to train a set of fasttext word vectors.\n",
        "# \n",
        "# The trained fasttext model is then saved so it can be loaded later\n",
        "# when building the neural networks embedding matrix.\n",
        "# ==========================================================================\n",
        "def build_ft_model(data_file_path=reddit_dataframe_path, text_data_save_path=reddit_text_path, wv_file=word_vector_file): \n",
        "\n",
        "    print(\"Reading reddit dataframe...\\n\")\n",
        "\n",
        "    # Read in the dataframe containing all potential training data.\n",
        "    reddit_df = pd.read_csv(data_file_path)\n",
        "\n",
        "    print(\"Grabbing text from dataframe... \\n\")\n",
        "\n",
        "    # Grab all of the text from the all potential training data file.\n",
        "    reddit_text = reddit_df.loc[:, 'all_text_data']\n",
        "\n",
        "    print(\"Saving text data to path...\")\n",
        "    print(text_data_save_path, \"\\n\")\n",
        "\n",
        "    # Save the text in a file that fasttext can access.\n",
        "    np.savetxt(text_data_save_path, reddit_text, fmt='%s')\n",
        "\n",
        "    print(\"Training fasttext model...\")\n",
        "\n",
        "    # Train the fasttext model.\n",
        "    fastText_model = fasttext.train_unsupervised(text_data_save_path, model='skipgram')\n",
        "\n",
        "    # Save the trained fasttext model. \n",
        "    print(\"Saving model to path...\")\n",
        "    print(wv_file, \"\\n\")\n",
        "    fastText_model.save_model(wv_file)\n",
        "    print(\"Model save complete!\")\n",
        "\n",
        "    return fastText_model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6MK43cUqnz2"
      },
      "source": [
        "# Commented out because the fasttext word vector model has already been created.\n",
        "# ft_model = build_ft_model()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIEVi23Y5upu"
      },
      "source": [
        "# Training a RNN \n",
        "\n",
        "Note: The build_and_train_rnn function allows for the choice of using fasttext word vectors in its embedding layer or having the network learn its own embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxVoLIVL2z3B"
      },
      "source": [
        "# ============================================================================================================================================\n",
        "# This function is used to build an embedding matrix that is populated with fasttext word vectors. This embedding matrix can be loaded into\n",
        "# the embedding layer of a recurrent neural network.\n",
        "#\n",
        "# This function performs the following:\n",
        "#\n",
        "# 1) Load a trained fasttext word vector model, which should be saved at the path given to the ft_file parameter.\n",
        "# 2) Instantiate a Keras TextVectorization layer using the max_vocab_size and max_sequence_length parameters.\n",
        "# 3) Load the training data stored at the path given to the training_data_csv_path parameter.\n",
        "# 4) Use the textvectorization layer to learn a vocabulary for the text in the training data.\n",
        "# 5) Use the learned vocabulary and the trained fasttext model to build an embedding matrix that contains fasttext \n",
        "#    word vectors for the top max_vocab_size most frequent words in the training data vocabulary.\n",
        "# ============================================================================================================================================\n",
        "def build_embedding_matrix(training_data_csv_path, max_vocab_size, max_sequence_length, ft_file=word_vector_file, embedding_dim=100):\n",
        "\n",
        "    # Load the trained fasttext model.\n",
        "    print(\"Loading fasttext word vector model...\\n\")\n",
        "    ft_model = fasttext.FastText.load_model(ft_file)\n",
        "\n",
        "    print(\"Instantiating keras text vectorizer...\\n\")\n",
        "    \n",
        "    # Instantiate the keras text vectorizer.\n",
        "    text_vectorization = TextVectorization(\n",
        "        max_tokens=max_vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=max_sequence_length,\n",
        "        split=\"whitespace\")\n",
        "\n",
        "    print(\"Reading in the training data file...\")\n",
        "    print(training_data_csv_path, \"\\n\")\n",
        "\n",
        "    # Load the training data\n",
        "    train_df = pd.read_csv(training_data_csv_path)\n",
        "\n",
        "    # Split into X and y\n",
        "    X_train = train_df.loc[:, 'all_text_data'].astype(str).to_numpy()\n",
        "    y_train = train_df.loc[:, 'subreddit'].to_numpy()\n",
        "\n",
        "    print(\"Having the text vectorizer learn the training data vocabulary...\\n\")\n",
        "\n",
        "    # Let the keras TextVectorization layer learn the vocabulary of the\n",
        "    # training data.\n",
        "    text_vectorization.adapt(X_train)\n",
        "\n",
        "    print(\"Getting vocab from text_vectorizer...\\n\")\n",
        "\n",
        "    # Get the training data vocab from the TextVectorization layer.\n",
        "    vocabulary = text_vectorization.get_vocabulary()\n",
        "\n",
        "    # Build a dictionary that maps each word in the vocabulary to a number\n",
        "    print(\"Building word index... word --->num.\")\n",
        "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "    # Create an empty embedding matrix (filled with all zeros).\n",
        "    print(\"Creating empty embedding matrix...\\n\")\n",
        "    embedding_matrix = np.zeros((max_vocab_size, embedding_dim))\n",
        "\n",
        "    print(\"Populating embedding matrix...\\n\")\n",
        "\n",
        "    # Loop over the vocabulary dictionary to populate the embedding matrix.\n",
        "    for word, index in list(word_index.items()):\n",
        "\n",
        "        # Add every word in the TextVectorizers vocab up to the max vocab size we decided on.\n",
        "        # Note: The TextVectorizer automatically has the most frequent words at the lower index values,\n",
        "        # so this will be the top max_vocab_size most frequent words.\n",
        "        if index < max_vocab_size:\n",
        "\n",
        "            # Get a vector for this word from the trained fasttext model.\n",
        "            embedding_vector = ft_model.get_word_vector(word)\n",
        "\n",
        "        # Add this vector to the embedding matrix.\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "        else:\n",
        "            print(f\"The fasttext model did not return a vector for word: {word}\")\n",
        "\n",
        "    print(\"Returning embedding matrix...\")\n",
        "    return embedding_matrix "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPq7gVM-20Hz"
      },
      "source": [
        "# After we finish the fasttext section, make sure these get imported in case there was any collision\n",
        "# with the same names being used in the fasttext library.\n",
        "from tensorflow.keras.models import save_model, load_model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DRzSrJYUnFu"
      },
      "source": [
        "# ============================================================================================================================================\n",
        "# This function is used to load the tensorflow integer datasets used when training the RNN.\n",
        "# ============================================================================================================================================\n",
        "def load_int_datasets(base_path, train_size, max_tokens):\n",
        "\n",
        "    print(\"loading the datasets\") \n",
        "    int_train_ds = load(base_path + f\"train{train_size}/train{train_size}_batch_32_maxTokens_{max_tokens}_maxLength_20_int_test_ds\")\n",
        "    int_val_ds = load(base_path + f\"train{train_size}/train{train_size}_batch_32_maxTokens_{max_tokens}_maxLength_20_int_val_ds\")\n",
        "    int_test_ds = load(base_path + f\"train{train_size}/train{train_size}_batch_32_maxTokens_{max_tokens}_maxLength_20_int_test_ds\")\n",
        "\n",
        "    print(\"Finished Loading the  datasets!\")\n",
        "    return int_train_ds, int_val_ds, int_test_ds"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3WiL1Zy20Ak"
      },
      "source": [
        "# ============================================================================================================================================\n",
        "# This function will perform the following:\n",
        "#\n",
        "# 1) Load tensorflow datasets that should be located at base_data_and_save_path/train{train_size}/ folder. \n",
        "#    The name of the dataset also indicates the size of the training set and the size of the vocabulary that was used when creating the dataset.\n",
        "#\n",
        "# 2) Create filepaths for saving the \"best\" model (checkpoint_save) and the model as it existed on the last epoch of training (final_save)\n",
        "#\n",
        "# If use_fasttext_embeddings\n",
        "# 3) Construct an embedding matrix of a vocabulary size defined by max_tokens, and fill the embedding matrix with fasttext word vectors.\n",
        "#\n",
        "# If not using fasttext embeddings\n",
        "# 3) instantiate an embedding layer that is trainable. \n",
        "# \n",
        "# 4) Instantiate a basic bidirectional RNN architecture.\n",
        "#\n",
        "# 5) Compile the model, and fit the network using the datasets loaded in step 1.\n",
        "#\n",
        "# 6) Evaluate the best model on the training, validation and test sets, and save the results to a .csv. \n",
        "# \n",
        "# ============================================================================================================================================\n",
        "def build_and_train_rnn(training_data_csv_path, train_set_size, epochs, checkpoint_save, max_tokens, use_fasttext_embeddings=False, final_save=None, dropout=0.5,\n",
        "                        optimizer='rmsprop', max_sequence_length=20, metrics=['accuracy'], loss=\"binary_crossentropy\", int_train_ds=None, int_test_ds=None,\n",
        "                        int_val_ds=None, ft_file=word_vector_file, embedding_dim=100, score_df=None,\n",
        "                        base_data_and_save_path=\"/content/drive/MyDrive/Programming/Colab Notebooks/General_Assembly/Project_3_NLP/data/iterating_data/\"): \n",
        "\n",
        "\n",
        "    # Load dataset (if they were not passed into the function as parameters).\n",
        "    if int_train_ds is None:\n",
        "        int_train_ds, int_val_ds, int_test_ds = load_int_datasets(base_data_and_save_path, train_set_size, max_tokens)\n",
        "\n",
        "    # Create save paths for the model.\n",
        "    # checkpoint_save is where the model will be saved each time there is a new \"best model\".\n",
        "    base_save_path = base_data_and_save_path + f\"/train{train_set_size}/\"\n",
        "    checkpoint_save = base_save_path + checkpoint_save\n",
        "\n",
        "    # Create the path to save the final model (the neural network at the last epoch in training).\n",
        "    if final_save is not None:\n",
        "        final_save = base_save_path + final_save\n",
        "\n",
        "    # If we are using fasttext embeddings in the embedding matrix\n",
        "    if use_fasttext_embeddings:\n",
        "\n",
        "        # Build the embedding matrix\n",
        "        embedding_matrix = build_embedding_matrix(training_data_csv_path=training_data_csv_path,\n",
        "                                                  max_vocab_size=max_tokens,\n",
        "                                                  max_sequence_length=max_sequence_length,\n",
        "                                                  ft_file=ft_file,\n",
        "                                                  embedding_dim=embedding_dim) \n",
        "\n",
        "        # Instantiate the embedding layer using the embedding matrix created above.\n",
        "        # Setting trainable=False means the network should not try to change the word embeddings we intialized it with. \n",
        "        embedding_layer = layers.Embedding(input_dim=max_tokens,\n",
        "                                           output_dim=embedding_dim,\n",
        "                                           embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "                                           input_length=max_sequence_length,\n",
        "                                           trainable=False,\n",
        "                                           mask_zero=True)\n",
        "        \n",
        "    # Else the network will need to learn its own embeddings.\n",
        "    else:\n",
        "\n",
        "        print(\"Instatiating the embedding layer.\")\n",
        "        print(\"This network will learn its own word embeddings.\")\n",
        "\n",
        "        embedding_layer = layers.Embedding(input_dim=max_tokens,\n",
        "                                           output_dim=embedding_dim,\n",
        "                                           input_length=max_sequence_length,\n",
        "                                           mask_zero=True)\n",
        "\n",
        "    print(\"Building model architecture\")\n",
        "    #--------------------------------------------------------\n",
        "    # Model architecture\n",
        "    #--------------------------------------------------------\n",
        "\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "    embedded = embedding_layer(inputs)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss,\n",
        "                  metrics=metrics)\n",
        "    \n",
        "\n",
        "    print(\"============================= Model Summary==================================\")\n",
        "    print(model.summary())\n",
        "    print(\"===============================================================\\n\\n\")\n",
        "\n",
        "    # Callback to save current \"best\" model\n",
        "    callbacks = [keras.callbacks.ModelCheckpoint(checkpoint_save,\n",
        "                                                 save_best_only=True)]\n",
        "\n",
        "    # Fit the model\n",
        "    print(\"Fitting model\")\n",
        "    model.fit(int_train_ds, validation_data=int_val_ds, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "\n",
        "    # If we want to save the model at the final epoch.\n",
        "    if final_save is not None:\n",
        "        save_model(model=model,\n",
        "                   filepath=final_save,  \n",
        "                   overwrite=True,\n",
        "                   include_optimizer=True)\n",
        "\n",
        "\n",
        "    # Load the best model\n",
        "    model = keras.models.load_model(checkpoint_save)\n",
        "\n",
        "    # If a dataframe was passed in to record the train, test and validation scores in, update that dataframe here.\n",
        "    if score_df is not None:\n",
        "        print(\"Updating scores...\\n\")\n",
        "        temp_dict = {\"train_accuracy\" : [], \"val_accuracy\" : [], \"test_accuracy\" : [], 'train_data_size' : []}\n",
        "        temp_dict['test_accuracy'].append(model.evaluate(int_test_ds)[1])\n",
        "        temp_dict['train_accuracy'].append(model.evaluate(int_train_ds)[1])\n",
        "        temp_dict['val_accuracy'].append(model.evaluate(int_val_ds)[1])\n",
        "        temp_dict['train_data_size'].append(train_set_size)\n",
        "\n",
        "        # Save the scores from this round in a temporary dataframe.\n",
        "        temp_df = pd.DataFrame(temp_dict)\n",
        "\n",
        "        # Concatenate this networks score to the dataframe tracking all scores.\n",
        "        score_df = pd.concat([score_df, temp_df], ignore_index=True)\n",
        "\n",
        "        # Save the updated score dataframe\n",
        "        score_df.to_csv(base_save_path + f\"BRNN_fasttext_SCORES{train_set_size}.csv\", index=False)\n",
        "\n",
        "    # Evaluate best model on the test set\n",
        "    print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "    # If we are using a score dataframe\n",
        "    if score_df is not None:\n",
        "\n",
        "        # Return the best model found and the score dataframe\n",
        "        return model, score_df\n",
        "    \n",
        "    else:\n",
        "\n",
        "        return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BogYrn_P8zdN"
      },
      "source": [
        "# ============================================================================================================================================\n",
        "# This function is used to train the neural network defined above multiple times with a list of training dataset sizes.\n",
        "# ============================================================================================================================================\n",
        "def train_rnn_with_multiple_training_set_sizes(train_sizes, epochs, max_tokens, use_fasttext_embeddings=False,\n",
        "                                               dropout=0.5, optimizer='rmsprop', max_sequence_length=20, metrics=['accuracy'],\n",
        "                                               loss=\"binary_crossentropy\", ft_file=None, embedding_dim=256, save_model_at_last_epoch=None,\n",
        "                                               base_data_and_save_path =\"/content/drive/MyDrive/Programming/Colab Notebooks/General_Assembly/Project_3_NLP/data/iterating_data/\"):\n",
        "    \n",
        "\n",
        "    # Create a list of the filepaths to training sets of each size in \"train_sizes\".\n",
        "    train_file_paths = [base_data_and_save_path + f\"train{size}/train_{size}.csv\" for size in train_sizes]\n",
        "\n",
        "    # Create an empty dataframe that will store the performance of the neural network after being trained on each dataset in the list above.\n",
        "    score_df = pd.DataFrame({\"train_accuracy\" : [], \"val_accuracy\" : [], \"test_accuracy\" : [], 'train_data_size' : []})\n",
        "\n",
        "    # For each datset we want to train the network with.\n",
        "    for train_size, file_path in zip(train_sizes, train_file_paths): \n",
        "\n",
        "        # Print the training file we are about to use.\n",
        "        print(f\"Training network with file at path ---> {file_path}\")\n",
        "\n",
        "        # The saved model filenames depend on if we used fasttext or not.\n",
        "        if use_fasttext_embeddings:\n",
        "\n",
        "            # Save the \"best\" model here, we always want to do this.\n",
        "            checkpoint_save = f\"fasttext_embeddings_BEST_brnn_model_trainsize_{train_size}.keras\"\n",
        "\n",
        "            # Option to save the last model or not. Helpful if we may want to train it more.\n",
        "            if save_model_at_last_epoch:\n",
        "                final_save = f\"fasttext_embeddings_FINAL_brnn_model_trainsize_{train_size}.keras\"\n",
        "            else:\n",
        "                final_save= None\n",
        "\n",
        "        else:\n",
        "            # Save the \"best\" model here, we always want to do this.\n",
        "            checkpoint_save = f\"learned_embeddings{embedding_dim}_BEST_brnn_model_trainsize_{train_size}.keras\"\n",
        "\n",
        "            # Option to save the last model or not. Helpful if we may want to train it more.\n",
        "            if save_model_at_last_epoch:\n",
        "                final_save = f\"learned_embeddings{embedding_dim}_FINAL_brnn_model_trainsize_{train_size}.keras\"\n",
        "            else:\n",
        "                final_save= None\n",
        "\n",
        "        # Train the RNN using the dataset read in above.\n",
        "        best_model, score_df = build_and_train_rnn(score_df=score_df,                          # Score df for tracking performance across different train set sizes\n",
        "                                                   training_data_csv_path=file_path,           # Train data path for building the embedding matrix         \n",
        "                                                   train_set_size=train_size,                  # Size of the training set, used in all the file names               \n",
        "                                                   max_tokens=max_tokens,                      # Max vocab size, used for building embedding layer                  \n",
        "                                                   epochs=epochs,                              # Number of epochs to train the network for          \n",
        "                                                   dropout=dropout,                            # Dropout percentage between LSTM and Dense layer\n",
        "                                                   optimizer=optimizer,                        # Optimizer to use when updating network weights\n",
        "                                                   max_sequence_length=max_sequence_length,    # Length of the longest sequence.\n",
        "                                                   metrics=metrics,\n",
        "                                                   loss=loss,\n",
        "                                                   ft_file=ft_file,\n",
        "                                                   embedding_dim=embedding_dim,\n",
        "                                                   base_data_and_save_path=base_data_and_save_path,\n",
        "                                                   checkpoint_save=checkpoint_save,\n",
        "                                                   final_save=final_save,\n",
        "                                                   use_fasttext_embeddings=use_fasttext_embeddings)  # True if using fastext, False for network to learn embeddings.\n",
        "    return score_df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8F92d8QENLp"
      },
      "source": [
        "train_sizes = [100, 1000, 3000, 5000, 10000, 30000, 50000, 150000, 200000, 300000, 400000, 500000]\n",
        "\n",
        "# Uncomment the function call below to train and evaluate performance of the RNN with fasttext\n",
        "# word embeddings using the training set sizes listed above.\n",
        "\n",
        "'''\n",
        "train_rnn_with_multiple_training_set_sizes(train_sizes=train_sizes,\n",
        "                                           epochs=20,\n",
        "                                           max_tokens=20000,\n",
        "                                           use_fasttext_embeddings=True,\n",
        "                                           save_model_at_last_epoch=True,\n",
        "                                           dropout=0.5,\n",
        "                                           optimizer='rmsprop',\n",
        "                                           max_sequence_length=20,\n",
        "                                           metrics=['accuracy'],\n",
        "                                           loss=\"binary_crossentropy\",\n",
        "                                           ft_file=word_vector_file,\n",
        "                                           embedding_dim=100,\n",
        "                   |                        base_data_and_save_path=\"/content/drive/MyDrive/Programming/Colab Notebooks/General_Assembly/Project_3_NLP/data/iterating_data/\")\n",
        "''';"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FscK4BZ6WD84"
      },
      "source": [
        "train_sizes = [100, 1000, 3000, 5000, 10000, 30000, 50000, 150000, 200000, 300000, 400000, 500000]\n",
        "\n",
        "# Uncomment the function call below to train and evaluate performance of the RNN \n",
        "# that will learn its own 256 dimensional word embeddings.\n",
        "\n",
        "'''\n",
        "train_rnn_with_multiple_training_set_sizes(train_sizes=train_sizes,\n",
        "                                           epochs=2,\n",
        "                                           max_tokens=20000,\n",
        "                                           use_fasttext_embeddings=False,\n",
        "                                           embedding_dim=256,\n",
        "                                           save_model_at_last_epoch=True,\n",
        "                                           dropout=0.5,\n",
        "                                           optimizer='rmsprop',\n",
        "                                           max_sequence_length=20,\n",
        "                                           metrics=['accuracy'],\n",
        "                                           loss=\"binary_crossentropy\",\n",
        "                                           ft_file=None,\n",
        "                                           base_data_and_save_path=\"/content/drive/MyDrive/Programming/Colab Notebooks/General_Assembly/Project_3_NLP/data/iterating_data/\")\n",
        "''';"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tvirfAkeCxM"
      },
      "source": [
        "### References:\n",
        "\n",
        "The idea to use a TextVectorization layer for creating the vocabulary, and the RNN architecture I implemented, were inspired by Francois Chollets book Deep Learning with Python"
      ]
    }
  ]
}