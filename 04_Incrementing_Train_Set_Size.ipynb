{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa23c6ec-d335-4313-a9b7-b710d6d94361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.data.experimental import save, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8684e8a-ca26-4861-bf98-a386fccb2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================\n",
    "# This function takes as input a dataframe containing reddit posts, and returns a filtered version of that\n",
    "# dataframe where the length of all posts is between word_count_min and word_count_max.\n",
    "# =========================================================================================================\n",
    "def filter_samples_by_word_count(df, word_count_max, word_count_min):\n",
    "    \n",
    "    df['all_text_data'] = df['all_text_data'].astype(str)\n",
    "    \n",
    "    df['word_count'] = df['all_text_data'].apply(lambda text: len(text.split()))\n",
    "    \n",
    "    # create word count filters\n",
    "    min_word_filter = (df['word_count'] >= word_count_min)\n",
    "    max_word_filter = (df['word_count'] <= word_count_max)\n",
    "    \n",
    "    df = df.loc[(min_word_filter) & (max_word_filter), :].copy(deep=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5dca33-4408-4648-a7a2-39424619c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================\n",
    "# This function references the 'created_utc' column to print the data and time of creation for the\n",
    "# newest and oldest post in the dataset.\n",
    "# =========================================================================================================\n",
    "def print_post_times(df):\n",
    "    \n",
    "    newest_post_time = np.max(df.loc[:, 'created_utc'].to_numpy())\n",
    "    oldest_post_time = np.min(df.loc[:, 'created_utc'].to_numpy())\n",
    "    \n",
    "    newest_post_string = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(newest_post_time))\n",
    "    oldest_post_string = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(oldest_post_time))\n",
    "    \n",
    "    print(\"===================================================\")\n",
    "    print(f\"Newest Post: {newest_post_string}\")\n",
    "    print(f\"Oldest Oldest: {oldest_post_string}\")\n",
    "    print(\"===================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8664d3c0-551e-47ac-aad1-5cd6b394967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================================\n",
    "# This function prints the number of duplicate samples in a dataframe. \n",
    "# Only the all_text_data and subreddit columns are considered when finding duplicates.\n",
    "# =========================================================================================================\n",
    "def duplicate_check(df):\n",
    "    \n",
    "    df = df.loc[:, ['all_text_data', 'subreddit']]\n",
    "    \n",
    "    print(\"===================================================\")\n",
    "    print(f\"Total Number of duplicates: {df.duplicated().sum()}\")\n",
    "    print(\"===================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea06282d-25aa-4420-843d-db7d32610b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================================================================\n",
    "# This function is used to print the number of unique words found in the training set after it is created in the \n",
    "# make_train_and_unused_sets function. \n",
    "#\n",
    "# Displaying this information could be helpful when considering what value to choose for max_tokens when building tensorflow datasets.\n",
    "# ===================================================================================================================================\n",
    "def print_num_unique_words(df):\n",
    "    \n",
    "    all_text = df.loc[:, 'all_text_data'].str.cat(sep=' ')\n",
    "    num_unique_words = len(list(set(all_text.split())))\n",
    "    \n",
    "    print(\"===================================================\")\n",
    "    print(f\"Number of unique words in the training set {num_unique_words}\")\n",
    "    print(\"===================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1857d6b3-b4b6-48f8-bfae-d9f94278a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================================\n",
    "# NOTE: This function only needs to be run once ever!\n",
    "# \n",
    "# This function is used to allocate the data into different sets to prepare for the process of iteratively retraining models with\n",
    "# larger and larger training sets and evaluating their performance.\n",
    "#\n",
    "# This function splits file containing the fully preprocessed posts (Processed_Through_Lemmatization_2073132_ALL_DATA.csv) into three datsets.\n",
    "#\n",
    "# 1) {min_words}_to_{max_words}_words_preprocessed_ALL_POTENTIAL_TRAIN.csv\n",
    "# 2) {min_words}_to_{max_words}_words_preprocessed_VALIDATION.csv\n",
    "# 3) {min_words}_to_{max_words}_words_preprocessed_TEST.csv\n",
    "#\n",
    "# In the above, min_words and max_words represent the minimum and maximum acceptable word length for any post. All posts outside this\n",
    "# range are discarded. For my experimentation I used min_words = 5 and max_words = 20 which left me with a little over 1 million\n",
    "# posts in total to experiement with. This process could be reperformed with a different min_word and max_word value if desired. \n",
    "# \n",
    "# The validation and test sets will contain 25k samples each. The all_potential_train set will contain the remainder of the\n",
    "# samples that meet the min_words to max_words criteria.\n",
    "# \n",
    "# The test set will be used to measure and compare the performance of models trained on different size training sets.\n",
    "# \n",
    "# The validation set will be used when training neural networks only. This set will be used as the validation_data\n",
    "# parameter when calling the tf.keras.fit() method. https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "# \n",
    "# The all_potential_train set will be pulled from to create larger and larger training sets as required to complete the \n",
    "# model performance vs training set size experimentation. \n",
    "#\n",
    "# ====================================================================================================================================\n",
    "\n",
    "def build_val_and_test_sets(max_words=20, min_words=5):\n",
    "    \n",
    "    # All preprocessed data.\n",
    "    full_preprocessed_df = pd.read_csv(\"./data/Processed/Processed_Through_Lemmatization_2073132_ALL_DATA.csv\")\n",
    "    \n",
    "    # Shuffle everything just for fun. (The data is initially very unshuffled, first half all crypto second half all wsb).\n",
    "    full_preprocessed_df = full_preprocessed_df.sample(frac=1, random_state = 42, axis='index').reset_index(drop=True)\n",
    "    \n",
    "    # Filter posts to only inlcude those that meet the min_words to max_words criteria.\n",
    "    filtered_df = filter_samples_by_word_count(full_preprocessed_df, word_count_max=max_words, word_count_min=min_words)\n",
    "    \n",
    "    # Only keep the features and target columns, as well as created_utc incase its interesting to look at later.\n",
    "    filtered_df.drop(columns=['selftext', 'title', 'word_count'], inplace=True)\n",
    "    \n",
    "    # Remove any post that is a duplicate (only consider the features and target when finding duplicates).\n",
    "    filtered_df = filtered_df.loc[ (filtered_df[['all_text_data', 'subreddit']].duplicated() == False), :]\n",
    "    \n",
    "    # Map the target to numeric values.\n",
    "    print(\"Mapping the target to numeric.... 0 for wsb and 1 for crypto.\\n\")\n",
    "    filtered_df['subreddit'] = [0 if reddit == 'wallstreetbets' else 1 for reddit in filtered_df['subreddit']]\n",
    "    \n",
    "    # Save a copy of all posts that were between min_words and max_words (no duplicates).\n",
    "    # This file is not necessarily needed for anything and is saved here only as a convenience. This would be the same data as\n",
    "    # concatenating the train, val and potential train sets back together.\n",
    "    filtered_df.to_csv(f\"./data/Processed/increment_train_size/{min_words}_to_{max_words}_words_preprocessed_ALL_POSTS.csv\", index=False)\n",
    "    \n",
    "    # Verify no duplicates are left in the dataset.\n",
    "    print(f\"Duplicates -- all {min_words} to {max_words} word posts.\")\n",
    "    duplicate_check(filtered_df)\n",
    "    \n",
    "    # Display the created time for the newest and oldest post.\n",
    "    print(\"Post times for the full dataset\")\n",
    "    print_post_times(filtered_df)\n",
    "    \n",
    "    X = filtered_df.drop(columns='subreddit')\n",
    "    y = filtered_df.loc[:, 'subreddit']\n",
    "    \n",
    "    # Split the data into a large set and a validation set of size 25k\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=25000)\n",
    "    \n",
    "    # Put the validation data into a df\n",
    "    val_df = pd.concat([X_val, y_val], axis=1)\n",
    "    \n",
    "    # Save validation data to a .csv\n",
    "    val_df.to_csv(f\"./data/Processed/increment_train_size/{min_words}_to_{max_words}_words_preprocessed_VALIDATION.csv\", index=False)\n",
    "    \n",
    "    # Split once more, into the \"potential training\" set and a test set of 25k\n",
    "    X_TRAIN, X_test, y_TRAIN, y_test = train_test_split(X_train, y_train, stratify=y_train, test_size=25000)\n",
    "    \n",
    "    # Put the test set into a dataframe.\n",
    "    test_df = pd.concat([X_test, y_test], axis=1)\n",
    "     \n",
    "    # Save the test set to .csv\n",
    "    test_df.to_csv(f\"./data/Processed/increment_train_size/{min_words}_to_{max_words}_words_preprocessed_TEST.csv\", index=False)\n",
    "    \n",
    "    # Create the \"Potential\" training set with the remainder of the data.\n",
    "    train_df = pd.concat([X_TRAIN, y_TRAIN], axis=1)\n",
    "    \n",
    "    # Save the potential training set to .csv\n",
    "    train_df.to_csv(f\"./data/Processed/increment_train_size/{min_words}_to_{max_words}_words_preprocessed_ALL_POTENTIAL_TRAIN.csv\", index=False)\n",
    "    \n",
    "    # Check for duplicates once more\n",
    "    print(\"Train set duplicates:\")\n",
    "    duplicate_check(train_df)\n",
    "    print(\"Validation set duplicates:\")\n",
    "    duplicate_check(val_df)\n",
    "    print(\"Test set duplicates:\")\n",
    "    duplicate_check(test_df)\n",
    "    print(\"\")\n",
    "       \n",
    "    # Check for post time distributions once more\n",
    "    print(\"Train set times:\")\n",
    "    print_post_times(train_df)\n",
    "    print(\"Validation set times:\")\n",
    "    print_post_times(val_df)\n",
    "    print(\"Test set times:\")\n",
    "    print_post_times(test_df)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Check for class distributions\n",
    "    print(f\"Train Distribution: \\n {train_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "    print(f\"Validation Distribution: \\n {val_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "    print(f\"Test Distribution: \\n {test_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "    \n",
    "    # Check dataset sizes:\n",
    "    print(f\"POTENTIAL Train set size {len(train_df.index)}\")\n",
    "    print(f\"Validation set size {len(val_df.index)}\")\n",
    "    print(f\"Test set size {len(test_df.index)}\")\n",
    "\n",
    "#build_val_and_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1bdbad-38cb-4097-b248-368c1f6e952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================================\n",
    "# This function is used to pull from the {min_words}_to_{max_words}_words_preprocessed_ALL_POTENTIAL_TRAIN.csv file to create\n",
    "# a dataset of a desired size.\n",
    "# ====================================================================================================================================\n",
    "def make_train_and_unused_sets(train_set_size, min_words=5, max_words=20):\n",
    "    \n",
    "    # Read in the dataset that contains all potential training data of posts between min_words and max_words.\n",
    "    potential_train = pd.read_csv(f\"./data/Processed/increment_train_size/{min_words}_to_{max_words}_words_preprocessed_ALL_POTENTIAL_TRAIN.csv\")\n",
    "    \n",
    "    \n",
    "    # Split the ALL_POTENTIAL_TRAIN data into a dataset of the desired size (train_set_size) and the remainder (the unused posts).\n",
    "    X = potential_train.drop(columns='subreddit')\n",
    "    y = potential_train['subreddit']\n",
    "    X_train, X_unused, y_train, y_unused = train_test_split(X, y, stratify=y, train_size=train_set_size)\n",
    "    \n",
    "    # Convert the training and unused posts into dataframes.\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    unused_df = pd.concat([X_unused, y_unused], axis=1)\n",
    "    \n",
    "    # Create two .csv files, one for the training set of size train_set_size. The other is the unused posts which will\n",
    "    # be of size ALL_POTENTIAL_TRAIN - train_set_size.\n",
    "    train_df.to_csv(f\"./data/Processed/increment_train_size/train{train_set_size}/train_{train_set_size}.csv\", index=False)\n",
    "    train_df.to_csv(f\"./data/Processed/increment_train_size/train{train_set_size}/unused_{train_set_size}.csv\", index=False)\n",
    "    \n",
    "    # Check for duplicates once more\n",
    "    print(\"Train set duplicates:\")\n",
    "    duplicate_check(train_df)\n",
    "    print(\"Unused set duplicates:\")\n",
    "    duplicate_check(unused_df)\n",
    "    print(\"\")\n",
    "       \n",
    "    # Check for post time distributions once more\n",
    "    print(\"Train set times:\")\n",
    "    print_post_times(train_df)\n",
    "    print(\"Unused set times:\")\n",
    "    print_post_times(unused_df)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Check for class distribution\n",
    "    print(f\"Train Distribution: \\n {train_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "    print(f\"Unused Distribution: \\n {unused_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "\n",
    "    # Check dataset size:\n",
    "    print(f\"Train set size {len(train_df.index)}\")\n",
    "    print(f\"Unused set size {len(unused_df.index)}\")\n",
    "    \n",
    "    # Print number of unique words\n",
    "    print_num_unique_words(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5689a2-4691-483b-94be-fa240e7940e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================================\n",
    "# This function takes as input a list of desired training set sizes. The function will then create folders for each training set size\n",
    "# at a path specified by ./data/Processed/increment_train_size/train{train_size}/ and will call the make_train_and_unused_sets\n",
    "# function to create the dataset.\n",
    "# ====================================================================================================================================\n",
    "def make_multiple_training_sets(train_set_sizes, base_path=\"./data/Processed/increment_train_size/\", min_words=5, max_words=20):\n",
    "    \n",
    "    for train_size in train_set_sizes:\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(base_path + f\"train{train_size}/\")\n",
    "            make_train_and_unused_sets(train_size, min_words=min_words, max_words=max_words)\n",
    "        except:\n",
    "            print(\"Path already exists!\")\n",
    "            print(base_path + f\"train{train_size}/\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15672bc0-ee5a-4853-b780-f3e071718a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set duplicates:\n",
      "===================================================\n",
      "Total Number of duplicates: 0\n",
      "===================================================\n",
      "\n",
      "Unused set duplicates:\n",
      "===================================================\n",
      "Total Number of duplicates: 0\n",
      "===================================================\n",
      "\n",
      "\n",
      "Train set times:\n",
      "===================================================\n",
      "Newest Post: 2021-06-26 18:59:34\n",
      "Oldest Oldest: 2014-12-08 08:19:32\n",
      "===================================================\n",
      "\n",
      "Unused set times:\n",
      "===================================================\n",
      "Newest Post: 2021-06-27 08:33:09\n",
      "Oldest Oldest: 2014-12-05 03:22:31\n",
      "===================================================\n",
      "\n",
      "\n",
      "Train Distribution: \n",
      " 0    0.5026\n",
      "1    0.4974\n",
      "Name: subreddit, dtype: float64\n",
      "\n",
      "Unused Distribution: \n",
      " 0    0.5026\n",
      "1    0.4974\n",
      "Name: subreddit, dtype: float64\n",
      "\n",
      "Train set size 20000\n",
      "Unused set size 885525\n",
      "===================================================\n",
      "Number of unique words in the training set 19888\n",
      "===================================================\n",
      "\n",
      "Train set duplicates:\n",
      "===================================================\n",
      "Total Number of duplicates: 0\n",
      "===================================================\n",
      "\n",
      "Unused set duplicates:\n",
      "===================================================\n",
      "Total Number of duplicates: 0\n",
      "===================================================\n",
      "\n",
      "\n",
      "Train set times:\n",
      "===================================================\n",
      "Newest Post: 2021-06-27 08:20:42\n",
      "Oldest Oldest: 2014-12-05 05:30:09\n",
      "===================================================\n",
      "\n",
      "Unused set times:\n",
      "===================================================\n",
      "Newest Post: 2021-06-27 08:33:09\n",
      "Oldest Oldest: 2014-12-05 03:22:31\n",
      "===================================================\n",
      "\n",
      "\n",
      "Train Distribution: \n",
      " 0    0.5026\n",
      "1    0.4974\n",
      "Name: subreddit, dtype: float64\n",
      "\n",
      "Unused Distribution: \n",
      " 0    0.5026\n",
      "1    0.4974\n",
      "Name: subreddit, dtype: float64\n",
      "\n",
      "Train set size 100000\n",
      "Unused set size 805525\n",
      "===================================================\n",
      "Number of unique words in the training set 53342\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sizes = [100 * n for n in range(1, 21)] + [250 * n for n in range(9, 20)] + [5000 * n for n in range(1, 101)]\n",
    "\n",
    "# Run this to make all the training set files.\n",
    "#make_multiple_training_sets(train_set_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d7300-b989-449e-9c08-7b4d0bbaa91b",
   "metadata": {},
   "source": [
    "## Tensorflow dataset section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9f0eeb5-3b35-492e-a515-065cd9d57b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================================\n",
    "# This function takes as input a list of pandas dataframes.\n",
    "# This function outputs a list of tensorflow datasets that is the result of return each dataframe into a tf dataset.\n",
    "# ====================================================================================================================================\n",
    "def build_tf_datasets(dfs, batch_size=32):\n",
    "    \n",
    "    tf_datasets = []\n",
    "    \n",
    "    for df in dfs: \n",
    "        \n",
    "        df = df.loc[:, ['all_text_data', 'subreddit']].copy(deep=True)\n",
    "         \n",
    "        # Make sure the text is all string datatype\n",
    "        df['all_text_data'] = df['all_text_data'].astype(str)\n",
    "        \n",
    "        # Make sure the subreddit column is all int datatypes\n",
    "        df['subreddit'] = df['subreddit'].astype('int64')\n",
    "        \n",
    "        # Create the tensorflow dataset with the appropriate batch size\n",
    "        ds = tf.data.Dataset.from_tensor_slices((tf.cast(df['all_text_data'], tf.string),\n",
    "                                                 tf.cast(df['subreddit'], tf.int64))).batch(batch_size)\n",
    "        \n",
    "        tf_datasets.append(ds)\n",
    "    \n",
    "    return tf_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0685f2c2-f9b0-44ce-91b2-81ba8608225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================================\n",
    "# This function takes as input the following values:\n",
    "#\n",
    "# 1) max_tokens: The maximum allowed vocab size\n",
    "#\n",
    "# 2) max_length: The maximum length of any sequence in the dataset. Using this value, the output integer datasets will have all sequences\n",
    "#    either padded or truncated to be exactly max_length. This means the tensors in the output dataset will be \n",
    "#    of shape [batch_size, output_sequence_length] regardless of how many tokens were in the original sequences. \n",
    "#    Note: This is only a valid TextVectorization option when output_mode = 'int' as we are doing here.\n",
    "#  \n",
    "# 3) A list of tensorflow string datasets set up as [train_ds, val_ds, test_ds].\n",
    "#\n",
    "# 4) A \"train_only_train_dataset\" which is the same information contained in train_ds except the target is not included.\n",
    "#\n",
    "# This function uses the keras TextVectorization the learn the vocabulary of the training dataset, and then creates\n",
    "# integer versions of the training, validation and test datasets which are then returned.\n",
    "# \n",
    "# ====================================================================================================================================\n",
    "def build_integer_datasets(max_tokens, max_length, datasets, train_only_train_dataset):\n",
    "    \n",
    "    # Unpack the string tensorflow datasets\n",
    "    train_ds, val_ds, test_ds = datasets\n",
    "\n",
    "    # Set up keras TextVectorization Layer\n",
    "    text_vectorization = TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=max_length)\n",
    "    \n",
    "    # Let the text vectorization layer learn the training datasets vocabulary.\n",
    "    text_vectorization.adapt(train_only_train_dataset)\n",
    "    \n",
    "    # Use the textvectorization layer to map strings to integers.\n",
    "    int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    \n",
    "    return [int_train_ds, int_val_ds, int_test_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd7c2e53-d73e-434f-8c5a-625792ddb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================================\n",
    "# This function takes the following inputs:\n",
    "#\n",
    "# 1 - 3) Dataframes containing training, validation and test datasets that we want to make tensorflow datasets for.\n",
    "#\n",
    "# 4) The desired batch size for the tensorflow dataset.\n",
    "#\n",
    "# 5) max_tokens: The maximum allowed vocab size.\n",
    "#\n",
    "# 6) max_length: The maximum allowed length of any sequence in the dataset.\n",
    "#\n",
    "# 7) train_size: The number of training examples in train_df. Used for making appropriate filenames only.\n",
    "#\n",
    "# 8 - 9) save_datasets, base_path ---> Optional parameters to determine if the datasets should be saved and where to save them. \n",
    "#\n",
    "# This function returns a dictionary containing all tensorflow datasets that were created. \n",
    "# ====================================================================================================================================\n",
    "def create_tensorflow_datasets_from_pandas(train_df, val_df, test_df, batch_size, max_tokens, max_length, train_size, save_datasets=True,\n",
    "                                           base_path=\"./data/Processed/deep/tf_data/\"):\n",
    "    \n",
    "    print(\"Creating the string tf datasets.\\n\")\n",
    "    \n",
    "    # Create the tensorflow datasets from the pandas dataframe.\n",
    "    train_ds, val_ds, test_ds = build_tf_datasets(dfs = [train_df, val_df, test_df], batch_size=batch_size)\n",
    "    \n",
    "    # Create a tensorflow dataset that only has the text data (not the target). We will use this for letting the TextVectorization\n",
    "    # layer the vocabulary for our text data.\n",
    "    text_only_train_ds = train_ds.map(lambda x, y : x)\n",
    "    \n",
    "    print(\"Creating the integer tf datasets.\\n\")\n",
    "    \n",
    "    # Create the integer tensorflow datasets. This is the result of using the TextVectorization layer to convert the strings in the \n",
    "    # tensorflow datasets on line 1 to unique integers. \n",
    "    int_train_ds, int_val_ds, int_test_ds = build_integer_datasets(datasets=[train_ds, val_ds, test_ds], train_only_train_dataset=text_only_train_ds,\n",
    "                                                                   max_tokens=max_tokens, max_length=max_length)\n",
    "    \n",
    "    # Create a dictionary of all the tf datasets that have been made.\n",
    "    datasets = {'train_ds' : train_ds,\n",
    "                'val_ds' : val_ds,\n",
    "                'test_ds' : test_ds,\n",
    "                'int_train_ds' : int_train_ds,\n",
    "                'int_val_ds' : int_val_ds,\n",
    "                'int_test_ds' : int_test_ds,\n",
    "                'text_only_train_ds' : text_only_train_ds}\n",
    "    \n",
    "    # Save the tensorflow datasets. \n",
    "    if save_datasets:\n",
    "        print(\"Saving the datasets\")\n",
    "        save(train_ds, path= base_path + f\"train{train_size}_batch_{batch_size}_train_ds\")\n",
    "        save(val_ds, path= base_path + f\"train{train_size}_batch_{batch_size}_val_ds\")\n",
    "        save(test_ds, path= base_path + f\"train{train_size}_batch_{batch_size}_test_ds\")\n",
    "        save(int_train_ds, path= base_path + f\"train{train_size}_batch_{batch_size}_maxTokens_{max_tokens}_maxLength_{max_length}_int_train_ds\")\n",
    "        save(int_val_ds, path= base_path + f\"train{train_size}_batch_{batch_size}_maxTokens_{max_tokens}_maxLength_{max_length}_int_val_ds\")\n",
    "        save(int_test_ds, path= base_path + f\"train{train_size}_batch_{batch_size}_maxTokens_{max_tokens}_maxLength_{max_length}_int_test_ds\")\n",
    "        save(text_only_train_ds, path= base_path + f\"train{train_size}_batch_{batch_size}_text_only_train_ds\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ad92ea7-cd4e-4a3f-b4e6-c888e22dc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================================================================\n",
    "# This function is used to efficiently call the create_tensorflow_datasets_from_pandas function multiple times.\n",
    "#\n",
    "# When passed a list of training_set_sizes (train_set_sizes) this function will attempt to read each sized dataset into a pandas\n",
    "# dataframe and then will call the create_tensorflow_dataset using the pandas dataframes as input.\n",
    "#\n",
    "# If datasets of sizes specified have train_set_sizes have not yet been saved as .csv files, then an error is printed and the\n",
    "# function returns.\n",
    "# ====================================================================================================================================\n",
    "def build_multiple_tf_datasets(train_set_sizes, save_datasets=True, batch_size=32, max_tokens=10000, max_length=20):\n",
    "    \n",
    "    # Loop over the list of train_set_sizes to create a tensorflow datasets for each. \n",
    "    for size in train_set_sizes:\n",
    "        try: \n",
    "            train_df = pd.read_csv(f\"./data/Processed/increment_train_size/train{size}/train_{size}.csv\")\n",
    "            val_df = pd.read_csv(f\"./data/Processed/increment_train_size/5_to_20_words_preprocessed_VALIDATION.csv\")\n",
    "            test_df = pd.read_csv(f\"./data/Processed/increment_train_size/5_to_20_words_preprocessed_TEST.csv\")\n",
    "        except:\n",
    "            print(f\"A dataset of size {size} has not been made yet.\")\n",
    "            return -1\n",
    "        \n",
    "        \n",
    "        tf_datasets = create_tensorflow_datasets_from_pandas(train_df, val_df, test_df, batch_size=batch_size, max_tokens=max_tokens, max_length=max_length,\n",
    "                                                             save_datasets=True, base_path=f\"./data/Processed/increment_train_size/train{size}/\",\n",
    "                                                             train_size=size) \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "930850f6-75f5-417d-82d9-f36014c62bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n",
      "Creating the string tf datasets.\n",
      "\n",
      "Creating the integer tf datasets.\n",
      "\n",
      "Saving the datasets\n"
     ]
    }
   ],
   "source": [
    "# train_sizes = [100, 500, 1000, 3000, 5000, 10000, 30000, 50000, 150000, 200000, 300000, 400000, 500000]\n",
    "build_multiple_tf_datasets(train_set_sizes=train_set_sizes, save_datasets=True, batch_size=32, max_tokens=20000, max_length=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
