{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80327230-2478-453e-897e-f36547469436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji-data in c:\\users\\braden\\anaconda3\\lib\\site-packages (0.1.6.1)\n"
     ]
    }
   ],
   "source": [
    "# Run this if the emoji-data library has not yet been installed.\n",
    "# ! pip install emoji-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e54150-5ee0-49d1-be46-491733db8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable used to keep a dictionary of any emojies identified during preprocessing that\n",
    "# we do not currently have a description for.\n",
    "global emojis_missing_descriptions\n",
    "emojis_missing_descriptions = {}\n",
    "emojis_missing_descriptions['emoji'] = []\n",
    "emojis_missing_descriptions['code'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb7ae62-5fe3-48ef-aa64-6c70f9719420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "from emoji_data import EmojiSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a738c8ce-5108-4294-816a-55d7cd736d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Monday, June 28, 2021\\r\\n\\r\\n09:0...</td>\n",
       "      <td>Market Events June 28 - July 2</td>\n",
       "      <td>1624808078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This made me super bullish for $AMZN. Also RIP...</td>\n",
       "      <td>1624807989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMC might be up to a big move this week!</td>\n",
       "      <td>1624807950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$COCP BLOCKBUSTER COVID ANTIVIRAL BIOTECH PLAY...</td>\n",
       "      <td>1624807898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I dont normally publish DD - but this made me ...</td>\n",
       "      <td>1624807864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                           selftext  \\\n",
       "0  wallstreetbets               Monday, June 28, 2021\\r\\n\\r\\n09:0...   \n",
       "1  wallstreetbets                                                NaN   \n",
       "2  wallstreetbets                                                NaN   \n",
       "3  wallstreetbets                                                NaN   \n",
       "4  wallstreetbets                                                NaN   \n",
       "\n",
       "                                               title  created_utc  \n",
       "0                     Market Events June 28 - July 2   1624808078  \n",
       "1  This made me super bullish for $AMZN. Also RIP...   1624807989  \n",
       "2           AMC might be up to a big move this week!   1624807950  \n",
       "3  $COCP BLOCKBUSTER COVID ANTIVIRAL BIOTECH PLAY...   1624807898  \n",
       "4  I dont normally publish DD - but this made me ...   1624807864  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the posts from the wallstreetbets subreddit.\n",
    "wsb_df = pd.read_csv(\"./data/Final/Clean_wallstreetbets_data_1125000_posts.csv\")\n",
    "wsb_df.name = \"WallStreetBets_DataFrame\"\n",
    "wsb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a74d0f-c207-4cec-b64d-b160db87cea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>All Nodes Ethereum 2.0 Services</td>\n",
       "      <td>1624735574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Is the Largest Difficulty Adjustment In Bitcoi...</td>\n",
       "      <td>1624735572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Binance leaving Ontario: Binance will handle E...</td>\n",
       "      <td>1624735503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>I own some purely because of the doge effect. ...</td>\n",
       "      <td>What are your thoughts on shib?</td>\n",
       "      <td>1624735442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miami Beach's most expensive penthouse just so...</td>\n",
       "      <td>1624735282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                           selftext  \\\n",
       "0  CryptoCurrency                                          [removed]   \n",
       "1  CryptoCurrency                                                NaN   \n",
       "2  CryptoCurrency                                                NaN   \n",
       "3  CryptoCurrency  I own some purely because of the doge effect. ...   \n",
       "4  CryptoCurrency                                                NaN   \n",
       "\n",
       "                                               title  created_utc  \n",
       "0                    All Nodes Ethereum 2.0 Services   1624735574  \n",
       "1  Is the Largest Difficulty Adjustment In Bitcoi...   1624735572  \n",
       "2  Binance leaving Ontario: Binance will handle E...   1624735503  \n",
       "3                    What are your thoughts on shib?   1624735442  \n",
       "4  Miami Beach's most expensive penthouse just so...   1624735282  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the posts from the CryptoCurrency subreddit\n",
    "crypto_df = pd.read_csv(\"./data/Final/Clean_CryptoCurrency_data_950000_posts.csv\")\n",
    "crypto_df.name = \"Crypto_DataFrame\"\n",
    "crypto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8757831e-60f5-4c0b-9659-8bb9274fbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================\n",
    "# Helper to the review posts function. Calculates several statistics related to the text\n",
    "# in a given set of posts.\n",
    "# =======================================================================================\n",
    "def calculate_statistics(df, column='selftext'):\n",
    "    \n",
    "    stats = {}\n",
    "    text_lists = {}\n",
    "    \n",
    "    stats['column_name'] = column\n",
    "    \n",
    "    # List of everything in the column\n",
    "    texts = [text_string for text_string in df.loc[:, column].to_numpy()]\n",
    "    text_lists['texts_all'] = texts\n",
    "    \n",
    "    # Length of every self text that is a string (not NaN)\n",
    "    text_lengths = [len(text) for text in texts if type(text) == str]\n",
    "    text_lists['str_text_lengths'] = text_lengths\n",
    "    \n",
    "    # Number of times the text is just the string '[removed]'\n",
    "    num_texts_removed = np.sum([1 if text == '[removed]' else 0 for text in texts])\n",
    "    stats['str_text_removed'] = num_texts_removed\n",
    "    \n",
    "    # The text for every selftext string that is not ['removed'] or NaN\n",
    "    valid_texts = [text for text in texts if text != '[removed]' and type(text) == str]\n",
    "    text_lists['valid_texts'] = valid_texts\n",
    "    \n",
    "    # The number of posts where the text isn't missing or ['removed']\n",
    "    num_valid_texts = len(valid_texts)\n",
    "    stats['num_valid_texts'] = num_valid_texts\n",
    "    \n",
    "    # Length of all text strings that are not missing or ['removed']\n",
    "    valid_text_lengths = [len(text) for text in valid_texts]\n",
    "    text_lists['valid_text_lengths'] = valid_text_lengths\n",
    "    \n",
    "    # Statistics \n",
    "    longest_text_string = np.max(valid_text_lengths)\n",
    "    stats['longest_text'] = longest_text_string\n",
    "    \n",
    "    shortest_text_string = np.min(valid_text_lengths)\n",
    "    stats['shortest_text'] = shortest_text_string\n",
    "    \n",
    "    avg_length_text_string = np.mean(valid_text_lengths)\n",
    "    stats['avg_text_length'] = avg_length_text_string\n",
    "    \n",
    "    median_length = np.median(valid_text_lengths)\n",
    "    stats['median_text_length'] = median_length\n",
    "    \n",
    "    twenty_fifth_percentile = np.quantile(a=valid_text_lengths, q=0.25)\n",
    "    stats['25th_perc_length'] = twenty_fifth_percentile\n",
    "    \n",
    "    seventy_fifth_percentile = np.quantile(a=valid_text_lengths, q=0.75)\n",
    "    stats['75th_perc_length'] = seventy_fifth_percentile\n",
    "    \n",
    "    ninetieth_percentile = np.quantile(a=valid_text_lengths, q=0.90)\n",
    "    stats['90th_perc_length'] = ninetieth_percentile\n",
    "    \n",
    "    num_text_missings = df[column].isna().sum()\n",
    "    stats['num_missings'] = num_text_missings\n",
    "    \n",
    "    print(f\"========================== {column} ==================================\")\n",
    "    print(f\"Number of missings in {column} strings: {num_text_missings}\")\n",
    "    print(f\"Number of {column} strings that are '[removed]': {num_texts_removed}\")\n",
    "    print(f\"Number of valid {column} strings: {num_valid_texts}\\n\")\n",
    "    \n",
    "    print(\">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\\n\")\n",
    "    \n",
    "    print(f\"Avg Length {column} string: {avg_length_text_string}\")\n",
    "    print(f\"Shortest {column} string: {shortest_text_string}\")\n",
    "    print(f\"25th percentile : {twenty_fifth_percentile}\")\n",
    "    print(f\"Median Length Self Text String: {median_length}\")\n",
    "    print(f\"75th percentile: {seventy_fifth_percentile}\")\n",
    "    print(f\"90th percentile: {ninetieth_percentile}\")\n",
    "    print(f\"Longest {column} string: {longest_text_string}\")\n",
    "    print(\"=========================================================================\\n\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc83fd7e-4098-4762-9dcb-ea8a2c635799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function concatenates the 'title' and 'selftext' columns into a\n",
    "# new column called 'all_text_data'\n",
    "def combine_columns(df):\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    df['selftext'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    df['selftext'] = [text if text != '[removed]' else \"\" for text in df['selftext']]\n",
    "    \n",
    "    df['title'].fillna(\"\", inplace=True)\n",
    "    \n",
    "    df['all_text_data'] = df['title'] + \" \" + df['selftext']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449b7a4d-2cfa-4787-b271-94abee13779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================================\n",
    "# This function has two separate purposes:\n",
    "# \n",
    "# 1. Call the \"combine_columns\" create a new \"all_text_data\" column that is the result\n",
    "#    of concatenating the 'title' and 'selftext' fields together.\n",
    "# \n",
    "# 2. Print various statistics that describe and help the user get some insight into\n",
    "#    the text data that is about to be preprocessed. \n",
    "# =======================================================================================\n",
    "def review_posts(df): \n",
    "    \n",
    "    print(f\"======================= Starting {df.name} Review ===============================================\\n\")\n",
    "    \n",
    "    print(f\"Number of rows in full dataframe {len(df.index)}\")\n",
    "    print(f\"Number of columns in full dataframe: {len(df.columns)}\\n\")\n",
    "    \n",
    "    # Remove all columns except 'subreddit', 'selftext', 'title', and 'created_utc'\n",
    "    # (If the file being collected came from the 00_Data_Collection notebook these)\n",
    "    # may already be removed.\n",
    "    clean_df = df.loc[:, ['subreddit', 'selftext', 'title', 'created_utc']].copy(deep=True)\n",
    "    \n",
    "    print(\"Creating smaller df that conly contains columns ---> subreddit, selftext, title, created_utc\")\n",
    "    print(f\"Number of columns in the smaller df ----> {len(clean_df.columns)}\\n\")\n",
    "    \n",
    "    # Get the newest and oldest posts epoch times\n",
    "    newest_post_time = np.max(clean_df.loc[:, 'created_utc'].to_numpy())\n",
    "    oldest_post_time = np.min(clean_df.loc[:, 'created_utc'].to_numpy())\n",
    "    \n",
    "    # https://stackoverflow.com/questions/12400256/converting-epoch-time-into-the-datetime\n",
    "    newest_post_string = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(newest_post_time))\n",
    "    oldest_post_string = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(oldest_post_time))\n",
    "    \n",
    "    print(\"========================== Duplicates ==================================\")\n",
    "    print(f\"Number of duplicates in full dataframe: {df.duplicated().sum()}\")\n",
    "    print(f\"Number of duplicates in smaller dataframe: {clean_df.duplicated().sum()}\")\n",
    "    print(\"=========================================================================\\n\")\n",
    "    \n",
    "    print(\"========================== Post Times ==================================\")\n",
    "    print(f\"Newest Post (Epoch Time): {newest_post_time}\")\n",
    "    print(f\"Newest Post (Local Time): {newest_post_string}\\n\")\n",
    "    print(f\"Oldest Post (Epoch Time): {oldest_post_time}\")\n",
    "    print(f\"Oldest Post (Local Time): {oldest_post_string}\")\n",
    "    print(\"=========================================================================\\n\")\n",
    "    \n",
    "    # Statistics on the selftext column\n",
    "    selftext_stats = calculate_statistics(clean_df, column='selftext')\n",
    "    \n",
    "    # Statistics on the title column\n",
    "    title_stats = calculate_statistics(clean_df, column='title')\n",
    "    \n",
    "    # Create a new 'alltext' column that is the combination of the 'title' + 'selftext' columns\n",
    "    clean_df = combine_columns(clean_df)\n",
    "    \n",
    "    # Statistics on the all-text column\n",
    "    all_text_stats = calculate_statistics(clean_df, column='all_text_data')\n",
    "        \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b04d24-9cf8-4b87-bab7-175d605bc724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Starting WallStreetBets_DataFrame Review ===============================================\n",
      "\n",
      "Number of rows in full dataframe 1125000\n",
      "Number of columns in full dataframe: 4\n",
      "\n",
      "Creating smaller df that conly contains columns ---> subreddit, selftext, title, created_utc\n",
      "Number of columns in the smaller df ----> 4\n",
      "\n",
      "========================== Duplicates ==================================\n",
      "Number of duplicates in full dataframe: 538\n",
      "Number of duplicates in smaller dataframe: 538\n",
      "=========================================================================\n",
      "\n",
      "========================== Post Times ==================================\n",
      "Newest Post (Epoch Time): 1624808078\n",
      "Newest Post (Local Time): 2021-06-27 08:34:38\n",
      "\n",
      "Oldest Post (Epoch Time): 1597869510\n",
      "Oldest Post (Local Time): 2020-08-19 13:38:30\n",
      "=========================================================================\n",
      "\n",
      "========================== selftext ==================================\n",
      "Number of missings in selftext strings: 482284\n",
      "Number of selftext strings that are '[removed]': 460917\n",
      "Number of valid selftext strings: 181799\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length selftext string: 479.7788051639448\n",
      "Shortest selftext string: 1\n",
      "25th percentile : 9.0\n",
      "Median Length Self Text String: 119.0\n",
      "75th percentile: 408.0\n",
      "90th percentile: 1085.0\n",
      "Longest selftext string: 39822\n",
      "=========================================================================\n",
      "\n",
      "========================== title ==================================\n",
      "Number of missings in title strings: 1\n",
      "Number of title strings that are '[removed]': 3\n",
      "Number of valid title strings: 1124996\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length title string: 50.40932856650157\n",
      "Shortest title string: 1\n",
      "25th percentile : 20.0\n",
      "Median Length Self Text String: 36.0\n",
      "75th percentile: 64.0\n",
      "90th percentile: 106.0\n",
      "Longest title string: 1191\n",
      "=========================================================================\n",
      "\n",
      "========================== all_text_data ==================================\n",
      "Number of missings in all_text_data strings: 0\n",
      "Number of all_text_data strings that are '[removed]': 0\n",
      "Number of valid all_text_data strings: 1125000\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length all_text_data string: 128.94100177777779\n",
      "Shortest all_text_data string: 1\n",
      "25th percentile : 24.0\n",
      "Median Length Self Text String: 44.0\n",
      "75th percentile: 85.0\n",
      "90th percentile: 182.0\n",
      "Longest all_text_data string: 39849\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_wsb_df = review_posts(wsb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd52c6c-3de4-4154-b5c8-e92fef61e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Starting Crypto_DataFrame Review ===============================================\n",
      "\n",
      "Number of rows in full dataframe 948853\n",
      "Number of columns in full dataframe: 4\n",
      "\n",
      "Creating smaller df that conly contains columns ---> subreddit, selftext, title, created_utc\n",
      "Number of columns in the smaller df ----> 4\n",
      "\n",
      "========================== Duplicates ==================================\n",
      "Number of duplicates in full dataframe: 140\n",
      "Number of duplicates in smaller dataframe: 140\n",
      "=========================================================================\n",
      "\n",
      "========================== Post Times ==================================\n",
      "Newest Post (Epoch Time): 1624735574\n",
      "Newest Post (Local Time): 2021-06-26 12:26:14\n",
      "\n",
      "Oldest Post (Epoch Time): 1417761426\n",
      "Oldest Post (Local Time): 2014-12-04 22:37:06\n",
      "=========================================================================\n",
      "\n",
      "========================== selftext ==================================\n",
      "Number of missings in selftext strings: 491072\n",
      "Number of selftext strings that are '[removed]': 240957\n",
      "Number of valid selftext strings: 216824\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length selftext string: 738.22324558167\n",
      "Shortest selftext string: 1\n",
      "25th percentile : 228.0\n",
      "Median Length Self Text String: 400.0\n",
      "75th percentile: 775.0\n",
      "90th percentile: 1577.0\n",
      "Longest selftext string: 40398\n",
      "=========================================================================\n",
      "\n",
      "========================== title ==================================\n",
      "Number of missings in title strings: 0\n",
      "Number of title strings that are '[removed]': 0\n",
      "Number of valid title strings: 948853\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length title string: 64.3516709121434\n",
      "Shortest title string: 1\n",
      "25th percentile : 33.0\n",
      "Median Length Self Text String: 54.0\n",
      "75th percentile: 78.0\n",
      "90th percentile: 119.0\n",
      "Longest title string: 339\n",
      "=========================================================================\n",
      "\n",
      "========================== all_text_data ==================================\n",
      "Number of missings in all_text_data strings: 0\n",
      "Number of all_text_data strings that are '[removed]': 0\n",
      "Number of valid all_text_data strings: 948853\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length all_text_data string: 234.04431034101174\n",
      "Shortest all_text_data string: 2\n",
      "25th percentile : 41.0\n",
      "Median Length Self Text String: 68.0\n",
      "75th percentile: 149.0\n",
      "90th percentile: 516.0\n",
      "Longest all_text_data string: 40445\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_crypto_df = review_posts(crypto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a9b99e2-bd95-4965-815d-d44778229150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function uses the emojis codepoint value to return its assoicated description, that is stored in \n",
    "# the desc_df dataframe. If not description is available, the emojis_missing_descriptions is updated\n",
    "# to track this. \n",
    "#\n",
    "# The verbose parameter is currently disabled (if False) as it is almost always undesireable to print\n",
    "# every emoji that gets replaced. This could be enabled again by replacing if False with if verbose. If \n",
    "# this is a desired function I think it is a better idea to reimplement the verbose functionality with\n",
    "# various levels, and make emoji printing the most extreme option (user wants a ton of messages). \n",
    "# ========================================================================================================\n",
    "def get_description(char, desc_df, verbose):\n",
    "    \n",
    "    global emojis_missing_descriptions\n",
    "    \n",
    "    # Convert the emoji to a codepoint value\n",
    "    code = str(hex(ord(char))).split('x')[1].upper()\n",
    "        \n",
    "    try: \n",
    "        # Reference desc_df to find the description associated with this emoji.\n",
    "        description = desc_df.loc[desc_df['codepoint'] == code, 'description'].to_numpy()[0]\n",
    "        description = \" \" + description + \" \"\n",
    "    except: \n",
    "        print(\"========================= Emojo Description Error =========================\")\n",
    "        print(f\"There is no description for emoji {char}\")\n",
    "        print(f\"Replacing the emoji with an empty string\")\n",
    "        print(\"===========================================================================\")\n",
    "        emojis_missing_descriptions['emoji'].append(char)\n",
    "        emojis_missing_descriptions['code'].append(code)\n",
    "        description = \"\"\n",
    "    \n",
    "    # This could be changed back to if verbose to allow each emoji that gets replaced to be printed out\n",
    "    # during preprocessing. I would only recommend doing this for small datasets, because printing each\n",
    "    # replacement for a large dataset will cause a significant slow down.\n",
    "    if False: \n",
    "        print(\"\\n>>>>>>>>>>>>>>>>>>>>>>> Replacing Emoji >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        print(f\"Emoji: {char}    Code:{code}\")\n",
    "        print(f\"Description: {description}\")\n",
    "        print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\")\n",
    "    \n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24065f26-7c74-4e2d-acab-1993ad8fddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function checks each character in a single posts text to determine if it is an emoji. Non-emoji\n",
    "# characters are passed through, and emoji characters are replaced with their associated description\n",
    "# via a call to the get_description() function\n",
    "# ========================================================================================================\n",
    "def emoji_processor(text, desc_df, verbose):\n",
    "    \n",
    "    # For each character, either pass it through if its not an emoji, or \n",
    "    # replace with the correct description if it is an emoji\n",
    "    char_list = [get_description(char, desc_df, verbose=verbose) if (char in EmojiSequence) else char for char in text]\n",
    "    \n",
    "    # Join the character list back together as a string. \n",
    "    new_text = \"\".join(char_list).strip()\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f50fa277-464f-4e52-88a2-d3067190afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function applies the emoji_processor to each post (row) in the dataframe to replace each emoji\n",
    "# with its associated description.\n",
    "# ========================================================================================================\n",
    "def replace_emojis_with_description(df, verbose):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n =) :) 8) <3 Replacing Emojis with descriptions <3 (8 (: (= \\n\")\n",
    "    \n",
    "    # Read in the file that has the correct description for each unique emoji.\n",
    "    descriptions_df = pd.read_excel(\"./support_data/COMPLETE_EMOJIS.xlsx\")\n",
    "    \n",
    "    # Replace emojis with their description. \n",
    "    df['all_text_data'] = df['all_text_data'].apply(lambda text : emoji_processor(text, descriptions_df, verbose=verbose))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e44ed28-9188-4621-a343-40eb9fcbf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function processes each word in a given post, and replaces any contractions with their \n",
    "# expanded form.\n",
    "#\n",
    "# Note: similar to the get_description, the verbose option has been disabled in this function. If this is\n",
    "# a desired functionality, simply replace if False with if verbose. Since printing contraction replacements\n",
    "# can lead to a significant slow down in preprocessing, a better option would be to reimplement the structure\n",
    "# of the \"verbose\" functionality to account for various levels. This should be included in the most\n",
    "# extreme option (user wants a lot of messages printed).\n",
    "# ========================================================================================================\n",
    "def contraction_processor(text, cont_df, verbose):\n",
    "    \n",
    "    # List of known contractions.\n",
    "    contractions = list(cont_df.loc[:, 'Contraction'].to_numpy())\n",
    "    \n",
    "    # List of words in the post\n",
    "    words = text.split()\n",
    "    \n",
    "    # Check if the words in the post are a known contraction. Replace any that are with their expanded form.\n",
    "    processed_words = [cont_df.loc[cont_df['Contraction'] == word, 'Expanded_Word'].to_numpy()[0].lower() if (word in contractions) else word for word in words]\n",
    "   \n",
    "    # If any changes were made, join the processed_words list back together into a string to return.\n",
    "    if words != processed_words:\n",
    "        new_text = \" \".join(processed_words).strip()\n",
    "    \n",
    "    # If no changes were made, just return the original text.\n",
    "    else:\n",
    "        new_text = text\n",
    "    \n",
    "    # Disabled verbose functionality, see note above.\n",
    "    if False and (words != processed_words):\n",
    "        \n",
    "        # The words we replaced are the ones in words but not in processed words\n",
    "        replaced_words = [word for word in words if (word not in processed_words)]\n",
    "        \n",
    "        # The words we replaced them with are the ones in processed_words but not in words\n",
    "        replacement_words = [cont_df.loc[cont_df['Contraction'] == word, 'Expanded_Word'].to_numpy()[0].lower() for word in replaced_words]\n",
    "        \n",
    "        print(\"/\\/\\/\\/\\/\\/\\/\\//\\/\\/\\/\\/\\/ Replacing Contractions /\\/\\/\\/\\/\\/\\/\\//\\/\\/\\/\\/\\/\")\n",
    "        print(f\"Replaced Words: {replaced_words}\")\n",
    "        print(f\"Replacement Words: {replacement_words}\")\n",
    "        print(\"/\\/\\/\\/\\/\\/\\/\\//\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\//\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\//\\/\\/\\/\\/\\/\\/ \\n\")\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec41dc8d-f6e0-4b51-9f5c-96db2ac768ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function calls the contraction_processor on each post to replace contractions with their expanded form\n",
    "# ========================================================================================================\n",
    "def replace_contractions_with_expanded_form(df, verbose):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n ~~~~~~~~~~~~~~~~~~~ Starting to replace contractions ~~~~~~~~~~~~~~~~~~~ \\n\")\n",
    "    \n",
    "    # Read the dataset of known contractions into a pandas dataframe. \n",
    "    contractions_df = pd.read_csv(\"./support_data/contractions.csv\")\n",
    "    \n",
    "    # Process each post to replace contractions with their expanded form.\n",
    "    df['all_text_data'] = df['all_text_data'].apply(lambda text : contraction_processor(text, contractions_df, verbose=verbose))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5742edb9-3a72-479f-94a8-c19955c78592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function processes the text for a single post to replace any instances of sms_speak with their\n",
    "# their standardized form. \n",
    "# ========================================================================================================\n",
    "def sms_speak_processor(text, sms_df, verbose):\n",
    "    \n",
    "    # Create a list of all words that are \"sms speak\" that we wish to replace with a more dictionary approved form.\n",
    "    sms_words = list(sms_df.loc[:, 'SMS_Wording'].to_numpy())\n",
    "    \n",
    "    # Split this post into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Check if each word is an sms word we want to replace, if it is, replace it with the correct form from the sms_df dataframe.\n",
    "    processed = [sms_df.loc[sms_df['SMS_Wording'] == word.rstrip('!.?'), 'Correct_Wording'].to_numpy()[0].lower() if (word.rstrip('!.?') in sms_words) else\n",
    "                 word for word in words]\n",
    "    \n",
    "    # Join the processed words back together to a single string.\n",
    "    if words != processed:\n",
    "        new_text = \" \".join(processed).strip()\n",
    "    else:\n",
    "        new_text = text\n",
    "    \n",
    "    # For ensuring the accuracy of the implementation only, inspect the changes that are being made.\n",
    "    if verbose and (words != processed):\n",
    "        \n",
    "        # The words we replaced are the ones in words but not in processed words\n",
    "        replaced_words = [word for word in words if (word not in processed)]\n",
    "        \n",
    "        # Get the replacement words\n",
    "        replacement_words = [sms_df.loc[sms_df['SMS_Wording'] == word, 'Correct_Wording'].to_numpy()[0].lower() for word in replaced_words]\n",
    "        \n",
    "        print(\"\\n============================ Replacing SMS Speak ============================\")\n",
    "        print(f\"Replaced Words: {replaced_words}\")\n",
    "        print(f\"Replacement Words: {replacement_words}\")\n",
    "        print(\"==================================================================================== \\n\")\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "848f3c28-3155-4e4c-97d9-c1665ba39544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function applies the sms_speak_processor to each post in the dataframe to replace all instances of\n",
    "# sms_speak with their standardized forms.\n",
    "# ========================================================================================================\n",
    "def replace_sms_speak(df, verbose):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n>>>>>>>>>>>>>>> Starting to replace SMS Speak <<<<<<<<<<<<<<<<<<<<\\n\")\n",
    "    \n",
    "    # Read in the dataset of known sms_speak terms.\n",
    "    sms_speak_df = pd.read_csv(\"./support_data/sms_speak.csv\")\n",
    "    \n",
    "    # Apply the sms_speak processor to each post.\n",
    "    df['all_text_data'] = df['all_text_data'].apply(lambda text : sms_speak_processor(text, sms_speak_df, verbose))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "466a35d3-3ea9-43e6-8506-a4d99a6c93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function can be used to remove any characters with unicode value greater than 127 \n",
    "# (i.e. not on the english keyboard) from a single post. \n",
    "# ========================================================================================================\n",
    "def remove_high_ord(text, verbose=False):\n",
    "    \n",
    "    # Discard anything left that is not a standard character (ord > 127)\n",
    "    clean_text = [char for char in text if (ord(char) <= 127)]\n",
    "    \n",
    "    # Create a single string containing all characters that were not discarded.\n",
    "    clean_text = \"\".join(clean_text).strip()\n",
    "    \n",
    "    # If the user wants messages displayed, print anything that this function removed.\n",
    "    if verbose:\n",
    "        removed_text = [char for char in text if (ord(char) >= 127)]\n",
    "        removed_text = \"\".join(removed_text).strip()\n",
    "        \n",
    "        if removed_text != \"\":\n",
    "        \n",
    "            print(\"========================================================================\")\n",
    "            print(f\"ORIGINAL TEXT:\\n {text}\\n\")\n",
    "            print(f\"CLEAN TEXT:\\n {clean_text}\\n\")\n",
    "            print(f\"REMOVED TEXT:\\n {removed_text}\\n\")\n",
    "            print(\"========================================================================\\n\")\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5abc811f-b237-405c-aadb-ae400eedc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the global list of emojis that are missing descriptions to update the full list of unique emojis.\n",
    "# Only needed during development of the emoji list, not needed for normal preprocessing.\n",
    "def missing_emojis():\n",
    "    \n",
    "    global emojis_missing_descriptions\n",
    "    \n",
    "    # Create a dataframe of the emojis we did not have descriptions for\n",
    "    missing_df = pd.DataFrame(emojis_missing_descriptions)\n",
    "    \n",
    "    # Indicate \"Not_Available\" in the dataframes description column.\n",
    "    missing_df['description'] = \"Not_Available\"\n",
    "    \n",
    "    # Drop duplicates to create a list of unique emojis that are missing.\n",
    "    missing_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    missing_df.rename(columns={'emoji' : 'unique_emojis', 'code':'codepoint', 'description':'description'}, inplace=True)\n",
    "    \n",
    "    complete_emoji_df = pd.read_excel(\"./support_data/COMPLETE_EMOJIS.xlsx\")\n",
    "    \n",
    "    combined_df = pd.concat([complete_emoji_df, missing_df], ignore_index=True)\n",
    "    \n",
    "    combined_df.to_excel(\"./support_data/ALL_UNIQUE_EMOJIS_IN_WORK.xlsx\", index=False)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f60199ff-adf3-43d0-b801-546cf8ae9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# This function takes in two dataframes containing posts from subreddits performs the following:\n",
    "#\n",
    "# 1. Combine to a single dataframe\n",
    "# 2. Drop any duplicate rows.\n",
    "# 3. Replace emojis with a text that describes the emojis meaning\n",
    "# 4. Replace SMS Speak\n",
    "# 5. Lowercase all text\n",
    "# 6. Replace contractions with expanded form\n",
    "# 7. Remove punctuation\n",
    "# 8 Remove Excessively long words (currently defined as > 25 chars) but this could become a parameter if desired.\n",
    "# 9. Remove any remaining characters with unicode value > 127.\n",
    "# 10. Save the processed dataframe to .csv\n",
    "# ========================================================================================================\n",
    "def preprocess(crypto_df, wsb_df, verbose, time_stats=True): \n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        start_time = time.time()\n",
    "        print(f\"Preprocess start time {start_time}\\n\\n\")\n",
    "        print(\">>>>>>>>>>>>> Combining DataFrames >>>>>>>>>>>>>>>\")\n",
    "        print(f\"Crypto_df rows: {len(crypto_df.index)}\")\n",
    "        print(f\"WSB df rows: {len(wsb_df.index)}\")\n",
    "    \n",
    "    # 1. Combine crypto and stocks into a single df\n",
    "    combined_df = pd.concat([crypto_df, wsb_df], ignore_index=True)\n",
    "    \n",
    "    # Give a name to the dataframe, its the nice thing to do. \n",
    "    combined_df.name = \"All_Reddit_Posts_Df\"\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Rows after concat: {len(combined_df.index)}\")\n",
    "        print(f\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"~~~~~~~~~~~~~~~~~~~~ Dropping Duplicate Rows ~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(f\"Duplicate before dropping: {combined_df.duplicated().sum()}\")\n",
    "    \n",
    "    # 2. Drop duplicate rows\n",
    "    combined_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Duplicates after dropping: {combined_df.duplicated().sum()}\")\n",
    "        print(f\"Rows after dropping duplicates: {len(combined_df.index)}\")\n",
    "        print(f\"Total Elapsed Time {time.time() - start_time}\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "    \n",
    "        print(\"====================================================================\")\n",
    "        print(\"Starting to replace emojis...\")\n",
    "    \n",
    "    # 3. Replace emojis with a text that describes the emojis meaning\n",
    "    combined_df = replace_emojis_with_description(combined_df, verbose=verbose)\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Finished replacing emojis, elapsed time is {time.time() - start_time}\")\n",
    "        print(\"====================================================================\\n\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(\"Starting to replace sms speak....\")\n",
    "    \n",
    "    # 4. Replace SMS Speak\n",
    "    combined_df = replace_sms_speak(combined_df, verbose=verbose)\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Finished replacing sms speak, elapsed time is {time.time() - start_time}\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "        print(\"------------------------- lower casing everything -------------------------------\")\n",
    "    \n",
    "    # 5. Lowercase everything\n",
    "    combined_df['all_text_data'] = combined_df['all_text_data'].apply(lambda text : text.lower())\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Finished lowercasing, elapsed time is {time.time() - start_time}\")\n",
    "        print(\"---------------------------------------------------------------------------------\\n\")\n",
    "        print(\"***********************************************************************************\")\n",
    "        print(\"Starting to replace contractions...\")\n",
    "    \n",
    "    # 6. Replace contractions with expanded form\n",
    "    combined_df = replace_contractions_with_expanded_form(combined_df, verbose=verbose)\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Finished replacing contractions, elapsed time is {time.time() - start_time}\")\n",
    "        print(\"***********************************************************************************\\n\")\n",
    "        print(\" !!!!!!!!!!!!!!!!!!!!!!!!!!!! Removing Punctuation !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    \n",
    "    # 7. Remove punctuation\n",
    "    combined_df['all_text_data'] = combined_df['all_text_data'].apply(lambda text : \"\".join([char for char in text if (char not in string.punctuation)]))\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Finished removing punctuation, elapsed time is {time.time() - start_time}\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "        print(\"len len len len -----> Removing execessively long words (25 chars+) <----- len len len len\")\n",
    "    \n",
    "    # 8 Remove Excessively long words. They are probably websites and not actual words. If they are too long, they are also probably too rare to be useful.\n",
    "    combined_df['all_text_data'] = combined_df['all_text_data'].apply(lambda text : \" \".join([word for word in text.split() if len(word) <= 25]))\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(f\"Finished removing long words, elapsed time is {time.time() - start_time}\")\n",
    "        print(\"----------------------------------------------------------------------------------\\n\")\n",
    "        print(\"128+ 128+ 128+ Removing any high ord characters that are left 128+ 128+ 128+ \\n\")\n",
    "    \n",
    "    \n",
    "    # 9. Remove anything characters left that have high ord (ord > 127). All emojis should be replaced already.\n",
    "    # This is going to be things like the euro symbol or special font apostrophies that don't get caught by str.punctuation. \n",
    "    combined_df['all_text_data'] = combined_df['all_text_data'].apply(lambda text : remove_high_ord(text, verbose=verbose))\n",
    "    \n",
    "    # If user wants to see outputs indicating how long each part of preprocessing took.\n",
    "    if time_stats:\n",
    "        print(\"Finished removing high ord characters!\")\n",
    "        print(f\"PREPROCESSING COMPLETE! total elapsed time {time.time() - start_time}\")\n",
    "        print(\"-----------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    # Get the total number of samples in the dataframe for use in the .csv filename. \n",
    "    num_samples = len(combined_df.index)\n",
    "    \n",
    "    # Save the processed dataframe to .csv\n",
    "    combined_df.to_csv(f\"./data/Processed/wsb_crypto_preprocessed_{num_samples}.csv\", index=False)\n",
    "    \n",
    "    # Update the file containing the list of unique emojis to include any that were identified\n",
    "    # to be missing descriptions in this round of processing.\n",
    "    # Commented out because this is only needed if there is a desire to add descriptions for\n",
    "    # additional new emojis. It is not needed for preprocessing to function.\n",
    "    # emoji_df = missing_emojis()\n",
    "    \n",
    "    df = review_posts(combined_df)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e5eba7-baad-4cde-ae6d-65ac4aeb954d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess start time 1625095479.124827\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>> Combining DataFrames >>>>>>>>>>>>>>>\n",
      "Crypto_df rows: 948853\n",
      "WSB df rows: 1125000\n",
      "Rows after concat: 2073853\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~ Dropping Duplicate Rows ~~~~~~~~~~~~~~~~~~~~\n",
      "Duplicate before dropping: 721\n",
      "Duplicates after dropping: 0\n",
      "Rows after dropping duplicates: 2073132\n",
      "Total Elapsed Time 14.119234323501587\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "====================================================================\n",
      "Starting to replace emojis...\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "========================= Emojo Description Error =========================\n",
      "There is no description for emoji \n",
      "Replacing the emoji with an empty string\n",
      "===========================================================================\n",
      "Finished replacing emojis, elapsed time is 310.0033423900604\n",
      "====================================================================\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Starting to replace sms speak....\n",
      "Finished replacing sms speak, elapsed time is 586.855836391449\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "------------------------- lower casing everything -------------------------------\n",
      "Finished lowercasing, elapsed time is 588.4267213344574\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "***********************************************************************************\n",
      "Starting to replace contractions...\n",
      "Finished replacing contractions, elapsed time is 844.3252828121185\n",
      "***********************************************************************************\n",
      "\n",
      " !!!!!!!!!!!!!!!!!!!!!!!!!!!! Removing Punctuation !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Finished removing punctuation, elapsed time is 873.4194486141205\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "len len len len -----> Removing execessively long words (25 chars+) <----- len len len len\n",
      "Finished removing long words, elapsed time is 882.7744452953339\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      "128+ 128+ 128+ Removing any high ord characters that are left 128+ 128+ 128+ \n",
      "\n",
      "Finished removing high ord characters!\n",
      "PREPROCESSING COMPLETE! total elapsed time 909.7702515125275\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "======================= Starting All_Reddit_Posts_Df Review ===============================================\n",
      "\n",
      "Number of rows in full dataframe 2073132\n",
      "Number of columns in full dataframe: 5\n",
      "\n",
      "Creating smaller df that conly contains columns ---> subreddit, selftext, title, created_utc\n",
      "Number of columns in the smaller df ----> 4\n",
      "\n",
      "========================== Duplicates ==================================\n",
      "Number of duplicates in full dataframe: 0\n",
      "Number of duplicates in smaller dataframe: 0\n",
      "=========================================================================\n",
      "\n",
      "========================== Post Times ==================================\n",
      "Newest Post (Epoch Time): 1624808078\n",
      "Newest Post (Local Time): 2021-06-27 08:34:38\n",
      "\n",
      "Oldest Post (Epoch Time): 1417761426\n",
      "Oldest Post (Local Time): 2014-12-04 22:37:06\n",
      "=========================================================================\n",
      "\n",
      "========================== selftext ==================================\n",
      "Number of missings in selftext strings: 0\n",
      "Number of selftext strings that are '[removed]': 0\n",
      "Number of valid selftext strings: 2073132\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length selftext string: 119.20208409305341\n",
      "Shortest selftext string: 0\n",
      "25th percentile : 0.0\n",
      "Median Length Self Text String: 0.0\n",
      "75th percentile: 0.0\n",
      "90th percentile: 271.0\n",
      "Longest selftext string: 40398\n",
      "=========================================================================\n",
      "\n",
      "========================== title ==================================\n",
      "Number of missings in title strings: 0\n",
      "Number of title strings that are '[removed]': 3\n",
      "Number of valid title strings: 2073129\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length title string: 56.791202091138565\n",
      "Shortest title string: 0\n",
      "25th percentile : 25.0\n",
      "Median Length Self Text String: 44.0\n",
      "75th percentile: 72.0\n",
      "90th percentile: 112.0\n",
      "Longest title string: 1191\n",
      "=========================================================================\n",
      "\n",
      "========================== all_text_data ==================================\n",
      "Number of missings in all_text_data strings: 0\n",
      "Number of all_text_data strings that are '[removed]': 0\n",
      "Number of valid all_text_data strings: 2073132\n",
      "\n",
      ">>>>>>>>>>>>>>> Stats below are for valid texts only <<<<<<<<<<<<<<<\n",
      "\n",
      "Avg Length all_text_data string: 176.99321702621927\n",
      "Shortest all_text_data string: 1\n",
      "25th percentile : 30.0\n",
      "Median Length Self Text String: 55.0\n",
      "75th percentile: 103.0\n",
      "90th percentile: 317.0\n",
      "Longest all_text_data string: 40445\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform preprocessing on the clean_crypto_df and clean_wsb_df dataframes.\n",
    "preprocessed_df = preprocess(crypto_df=clean_crypto_df, wsb_df=clean_wsb_df, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c1f8fd0-89b2-4ca3-86d9-0c625d5dcec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>all_text_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td></td>\n",
       "      <td>All Nodes Ethereum 2.0 Services</td>\n",
       "      <td>1624735574</td>\n",
       "      <td>all nodes ethereum 20 services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td></td>\n",
       "      <td>Is the Largest Difficulty Adjustment In Bitcoi...</td>\n",
       "      <td>1624735572</td>\n",
       "      <td>is the largest difficulty adjustment in bitcoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td></td>\n",
       "      <td>Binance leaving Ontario: Binance will handle E...</td>\n",
       "      <td>1624735503</td>\n",
       "      <td>binance leaving ontario binance will handle et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>I own some purely because of the doge effect. ...</td>\n",
       "      <td>What are your thoughts on shib?</td>\n",
       "      <td>1624735442</td>\n",
       "      <td>what are your thoughts on shib i own some pure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td></td>\n",
       "      <td>Miami Beach's most expensive penthouse just so...</td>\n",
       "      <td>1624735282</td>\n",
       "      <td>miami beachs most expensive penthouse just sol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit                                           selftext  \\\n",
       "0  CryptoCurrency                                                      \n",
       "1  CryptoCurrency                                                      \n",
       "2  CryptoCurrency                                                      \n",
       "3  CryptoCurrency  I own some purely because of the doge effect. ...   \n",
       "4  CryptoCurrency                                                      \n",
       "\n",
       "                                               title  created_utc  \\\n",
       "0                    All Nodes Ethereum 2.0 Services   1624735574   \n",
       "1  Is the Largest Difficulty Adjustment In Bitcoi...   1624735572   \n",
       "2  Binance leaving Ontario: Binance will handle E...   1624735503   \n",
       "3                    What are your thoughts on shib?   1624735442   \n",
       "4  Miami Beach's most expensive penthouse just so...   1624735282   \n",
       "\n",
       "                                       all_text_data  \n",
       "0                     all nodes ethereum 20 services  \n",
       "1  is the largest difficulty adjustment in bitcoi...  \n",
       "2  binance leaving ontario binance will handle et...  \n",
       "3  what are your thoughts on shib i own some pure...  \n",
       "4  miami beachs most expensive penthouse just sol...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
