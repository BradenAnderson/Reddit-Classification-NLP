{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc58ddd5-26b7-4bc4-bc76-066d161c3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, SCORERS, multilabel_confusion_matrix, make_scorer, roc_curve, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9827e0c1-097c-48e2-bde4-e0efb8f0dfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>all_text_data</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1532973281</td>\n",
       "      <td>tron justin sun stop short reveal secret project</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1550248670</td>\n",
       "      <td>reuters hsbc forex trading cost cut sharply bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1612224578</td>\n",
       "      <td>put expire 5th think clown brain read</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1543091767</td>\n",
       "      <td>toilet french start name trone concidence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1571098829</td>\n",
       "      <td>apollo 11 jpeg yes roomba ltolgt microsoft wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc                                      all_text_data  subreddit\n",
       "0   1532973281   tron justin sun stop short reveal secret project          1\n",
       "1   1550248670  reuters hsbc forex trading cost cut sharply bl...          1\n",
       "2   1612224578              put expire 5th think clown brain read          0\n",
       "3   1543091767          toilet french start name trone concidence          1\n",
       "4   1571098829  apollo 11 jpeg yes roomba ltolgt microsoft wor...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the test data used to evaluate models.\n",
    "test_df = pd.read_csv(\"./data/Processed/increment_train_size/5_to_20_words_preprocessed_TEST.csv\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5d93b-681f-4b06-83a4-3cf45db6fa22",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56bf6a3-de65-4d75-8801-eda441ee9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_score(test_df, model, params):\n",
    "    \n",
    "    X_test = test_df['all_text_data']\n",
    "    y_true = test_df['subreddit']\n",
    "    \n",
    "    print(\"==================================================================================================\")\n",
    "    print(f\"Model params:\\n {params}\\n\")\n",
    "    print(f\"Test set score: {model.score(X_test, y_true)}\")\n",
    "    print(\"==================================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ebe989-48df-4295-859a-d7365aff9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# Helper function used to convert a gridsearch output into a pandas dataframe with the columns formatted the way I like them. \n",
    "# ===========================================================================================================================\n",
    "def gs_to_clean_df(search_results, keep_split = False, keep_std = False, keep_time = False, keep_params = False, sort_by=None, make_RMSE=False):\n",
    "\n",
    "    gs_results_df = pd.DataFrame(search_results)\n",
    "\n",
    "    gs_result_columns = list(gs_results_df.columns)\n",
    "    throw_away_columns = []\n",
    "    columns_to_keep = []\n",
    "    columns_renamed = []\n",
    "    valid_metrics = []\n",
    "\n",
    "    for column_name in gs_result_columns: \n",
    "\n",
    "        if column_name.startswith('split'):\n",
    "            if keep_split == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif 'time' in column_name: \n",
    "            if keep_time == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif column_name.startswith('std'):\n",
    "            if keep_std == True: \n",
    "                columns_to_keep.append(column_name)\n",
    "            else: \n",
    "                throw_away_columns.append(column_name)\n",
    "        elif column_name == 'params':\n",
    "            if keep_params == True:\n",
    "                columns_to_keep.append(column_name)\n",
    "            else:\n",
    "                throw_away_columns.append(column_name)\n",
    "        else: \n",
    "            columns_to_keep.append(column_name)\n",
    "\n",
    "    gs_results_df.drop(labels=throw_away_columns, axis='columns', inplace=True)\n",
    "    renaming_dict = {}\n",
    "\n",
    "    for column_name in columns_to_keep: \n",
    "        name = \"\"\n",
    "\n",
    "        if column_name.startswith('param') and column_name != 'params': \n",
    "            name_components = column_name.split('__')\n",
    "\n",
    "            name_components = name_components[1:]\n",
    "\n",
    "            for component in name_components:\n",
    "                name = name + '_' + component \n",
    "                \n",
    "            name = name.lstrip('_')\n",
    "            \n",
    "        elif '_test' in column_name:\n",
    "            name = column_name.replace('_test', '')\n",
    "\n",
    "        renaming_dict[column_name] = name\n",
    "\n",
    "        if name.startswith('rank') or name.startswith('mean'):\n",
    "            valid_metrics.append(name)\n",
    "\n",
    "    gs_results_df.rename(columns=renaming_dict, inplace=True)\n",
    "\n",
    "    if sort_by in valid_metrics:\n",
    "        gs_results_df.sort_values(by=sort_by, inplace=True, ignore_index=True)\n",
    "\n",
    "    if make_RMSE:\n",
    "        gs_results_df['mean_RMSE'] = (abs(gs_results_df['mean_MSE'])) ** (1/2)\n",
    "\n",
    "    return gs_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bee35e-5ed8-4b8d-9e18-a8edb4693b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# Print the number of words in the shortest and longest post in the dataset.\n",
    "# ===============================================================================================\n",
    "def print_word_counts(df):\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    df['all_text_data'] = df['all_text_data'].astype(str)\n",
    "    \n",
    "    df['word_count'] = df['all_text_data'].apply(lambda text: len(text.split()))\n",
    "    \n",
    "    max_words = df['word_count'].max()\n",
    "    min_words = df['word_count'].min()\n",
    "    \n",
    "    print(f\"Maximum length post: {max_words} words\")\n",
    "    print(f\"Minimum length post: {min_words} words\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4530578-bdd1-4f1f-8ead-0e83e7dcec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================================\n",
    "# This function takes as input a dataframe containing reddit posts and returns the text data and\n",
    "# target split apart.\n",
    "#\n",
    "# This function also performs some final checks to make sure the data is correct prior to\n",
    "# attempting to build any models with it.\n",
    "# ===============================================================================================\n",
    "def data_check(train_df):\n",
    "    \n",
    "    print(f\"Distribution:\\n {train_df['subreddit'].value_counts(normalize=True)}\\n\")\n",
    "    \n",
    "    print(f\"Missing values: {train_df['all_text_data'].isna().sum()}\\n\")\n",
    "            \n",
    "    X = train_df.loc[:, 'all_text_data'].to_numpy()\n",
    "    y = train_df.loc[:, 'subreddit'].to_numpy().ravel() \n",
    "    \n",
    "    print(f\"Number of duplicates in training set: {train_df.duplicated().sum()}\\n\")\n",
    "    print(f\"Number of samples in training set: {len(train_df.index)}\\n\")\n",
    "    \n",
    "    print_word_counts(train_df)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22310fb8-e695-4036-8361-ba94228fd1dc",
   "metadata": {},
   "source": [
    "## 5000 sample grid searches \n",
    "\n",
    "Data only includes posts 5 to 20 words in length.\n",
    "\n",
    "Only given 5000 samples of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b007cb6a-df36-4c7b-a5d0-d237cb11f665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      " 0    0.5026\n",
      "1    0.4974\n",
      "Name: subreddit, dtype: float64\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "Number of duplicates in training set: 0\n",
      "\n",
      "Number of samples in training set: 5000\n",
      "\n",
      "Maximum length post: 20 words\n",
      "Minimum length post: 5 words\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/Processed/increment_train_size/train5000/train_5000.csv\")\n",
    "\n",
    "X, y = data_check(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a753fa53-961e-472e-8926-aa6cb01c6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tree_num_distribution = [100, 300, 500]\n",
    "max_depth = [-1, 5, 9]\n",
    "\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"boosted_trees\", LGBMClassifier(silent=False, random_state=42))])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,1), (1,2)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'boosted_trees__n_estimators' : tree_num_distribution, \n",
    "                   'boosted_trees__max_depth' : max_depth,\n",
    "                   'boosted_trees__subsample' : [0.8, 1.0],                    # Enable random selection of training cases (rows).\n",
    "                   'boosted_trees__colsample_bytree' : [0.8, 1.0],             # Percentage of features to randomly consider at each split.\n",
    "                   'boosted_trees__learning_rate' : [0.003, 0.01, 0.05]}]\n",
    "\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=5, verbose=3)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs1.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244c8d8f-167a-4741-8331-8d2684b19cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range colsample_bytree learning_rate max_depth  \\\n",
       "0     word         8761      (1, 2)              1.0          0.05         9   \n",
       "1     word         8761      (1, 2)              1.0          0.05         9   \n",
       "2     word         8761      (1, 2)              1.0          0.05         5   \n",
       "3     word         8761      (1, 2)              1.0          0.05         5   \n",
       "4     word         8761      (1, 2)              0.8          0.05         5   \n",
       "\n",
       "  n_estimators subsample  mean_score  rank_score  \n",
       "0          300       1.0      0.8822           1  \n",
       "1          300       0.8      0.8822           1  \n",
       "2          500       1.0      0.8820           3  \n",
       "3          500       0.8      0.8820           3  \n",
       "4          500       1.0      0.8814           5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs1.pkl'\n",
    "\n",
    "# Read in the model file created by the above gridsearch.\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results1 = pickle.load(file)\n",
    "    \n",
    "search_results1 = gs_results1.cv_results_\n",
    "top_estimator1 = gs_results1.best_estimator_\n",
    "top_score1 = gs_results1.best_score_\n",
    "top_parameters1 = gs_results1.best_params_\n",
    "\n",
    "# Display the gridsearch results\n",
    "gs1_df = gs_to_clean_df(search_results1, sort_by='rank_score')\n",
    "gs1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c89d55-1fad-43db-a372-621adc3d4398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tfidf_Vect__analyzer': 'word',\n",
       " 'Tfidf_Vect__max_features': 8761,\n",
       " 'Tfidf_Vect__ngram_range': (1, 2),\n",
       " 'boosted_trees__colsample_bytree': 1.0,\n",
       " 'boosted_trees__learning_rate': 0.05,\n",
       " 'boosted_trees__max_depth': 9,\n",
       " 'boosted_trees__n_estimators': 300,\n",
       " 'boosted_trees__subsample': 0.8}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_parameters1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46b3a30-3bbd-49d5-9b92-9d8686f41888",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tree_num_distribution = [200, 300, 500, 1000]\n",
    "max_depth = [8, 9, 11]\n",
    "\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"boosted_trees\", LGBMClassifier(silent=False, random_state=42))])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,1), (1,2)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'boosted_trees__n_estimators' : tree_num_distribution, \n",
    "                   'boosted_trees__max_depth' : max_depth,\n",
    "                   'boosted_trees__subsample' : [0.8, 1.0],                    # Enable random selection of training cases (rows).\n",
    "                   'boosted_trees__colsample_bytree' : [0.8, 1.0],             # Percentage of features to randomly consider at each split.\n",
    "                   'boosted_trees__learning_rate' : [0.008, 0.03, 0.05, 0.08]}]\n",
    "\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=5, verbose=3)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs2.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f2b5df7-57d8-4511-8879-5add32c79bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9</td>\n",
       "      <td>500</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8826</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range colsample_bytree learning_rate max_depth  \\\n",
       "0     word         8761      (1, 2)              1.0          0.08        11   \n",
       "1     word         8761      (1, 2)              0.8          0.08         9   \n",
       "2     word         8761      (1, 2)              1.0          0.08        11   \n",
       "3     word         8761      (1, 2)              0.8          0.08         9   \n",
       "4     word         8761      (1, 2)              1.0          0.03         9   \n",
       "\n",
       "  n_estimators subsample  mean_score  rank_score  \n",
       "0          200       0.8      0.8832           1  \n",
       "1          200       0.8      0.8832           1  \n",
       "2          200       1.0      0.8832           1  \n",
       "3          200       1.0      0.8832           1  \n",
       "4          500       0.8      0.8826           5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs2.pkl'\n",
    "\n",
    "# Read in the model file created by the above gridsearch.\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results2 = pickle.load(file)\n",
    "    \n",
    "search_results2 = gs_results2.cv_results_\n",
    "top_estimator2 = gs_results2.best_estimator_\n",
    "top_score2 = gs_results2.best_score_\n",
    "top_parameters2 = gs_results2.best_params_\n",
    "\n",
    "# Display the gridsearch results\n",
    "gs2_df = gs_to_clean_df(search_results2, sort_by='rank_score')\n",
    "gs2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4a3cd7-9eae-4508-8632-6c7ed7d3a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tree_num_distribution = [150, 200, 250, 300, 800]\n",
    "max_depth = [10, 11, 12, 13, 14]\n",
    "\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"boosted_trees\", LGBMClassifier(silent=False, random_state=42))])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,2), (1,3)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'boosted_trees__n_estimators' : tree_num_distribution, \n",
    "                   'boosted_trees__max_depth' : max_depth,\n",
    "                   'boosted_trees__subsample' : [0.7, 0.8, 0.9],                    # Enable random selection of training cases (rows).\n",
    "                   'boosted_trees__colsample_bytree' : [0.8, 0.9, 1.0],             # Percentage of features to randomly consider at each split.\n",
    "                   'boosted_trees__learning_rate' : [0.01, 0.03, 0.07, 0.08, 0.1]}]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=5, verbose=3)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs3.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e51ca1a-c05c-40cd-9837-f5b7dbc8deb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range colsample_bytree learning_rate max_depth  \\\n",
       "0     word         8761      (1, 2)              1.0           0.1        11   \n",
       "1     word         8761      (1, 2)              1.0           0.1        11   \n",
       "2     word         8761      (1, 2)              1.0           0.1        11   \n",
       "3     word         8761      (1, 2)              1.0          0.08        11   \n",
       "4     word         8761      (1, 2)              1.0          0.08        11   \n",
       "\n",
       "  n_estimators subsample  mean_score  rank_score  \n",
       "0          150       0.9      0.8844           1  \n",
       "1          150       0.8      0.8844           1  \n",
       "2          150       0.7      0.8844           1  \n",
       "3          200       0.7      0.8832           4  \n",
       "4          200       0.8      0.8832           4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs3.pkl'\n",
    "\n",
    "# Read in the model file created by the above gridsearch.\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results3 = pickle.load(file)\n",
    "    \n",
    "search_results3 = gs_results3.cv_results_\n",
    "top_estimator3 = gs_results3.best_estimator_\n",
    "top_score3 = gs_results3.best_score_\n",
    "top_parameters3 = gs_results3.best_params_\n",
    "\n",
    "# Display the gridsearch results\n",
    "gs3_df = gs_to_clean_df(search_results3, sort_by='rank_score')\n",
    "gs3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12eba794-c1ce-49b0-870d-954767b8b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tree_num_distribution = [1000, 1100, 1200]\n",
    "max_depth = [10, 11, 12]\n",
    "\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"boosted_trees\", LGBMClassifier(silent=False, random_state=42))])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,2)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'boosted_trees__n_estimators' : tree_num_distribution, \n",
    "                   'boosted_trees__max_depth' : max_depth,\n",
    "                   'boosted_trees__subsample' : [0.8, 0.9, 1.0],                    # Enable random selection of training cases (rows).\n",
    "                   'boosted_trees__colsample_bytree' : [0.8, 0.9, 1.0],             # Percentage of features to randomly consider at each split.\n",
    "                   'boosted_trees__learning_rate' : [0.005, 0.01, 0.05]}]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=5, verbose=3)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs4.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585246b0-d571-47ab-87ca-7bf2cf7a98bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>12</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>12</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>12</td>\n",
       "      <td>1200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>12</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>12</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range colsample_bytree learning_rate max_depth  \\\n",
       "0     word         8761      (1, 2)              1.0          0.01        12   \n",
       "1     word         8761      (1, 2)              1.0          0.01        12   \n",
       "2     word         8761      (1, 2)              1.0          0.01        12   \n",
       "3     word         8761      (1, 2)              0.8          0.01        12   \n",
       "4     word         8761      (1, 2)              0.8          0.01        12   \n",
       "\n",
       "  n_estimators subsample  mean_score  rank_score  \n",
       "0         1200       0.9      0.8818           1  \n",
       "1         1200       0.8      0.8818           1  \n",
       "2         1200       1.0      0.8818           1  \n",
       "3         1200       0.8      0.8814           4  \n",
       "4         1200       0.9      0.8814           4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs4.pkl'\n",
    "\n",
    "# Read in the model file created by the above gridsearch.\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results4 = pickle.load(file)\n",
    "    \n",
    "search_results4 = gs_results4.cv_results_\n",
    "top_estimator4 = gs_results4.best_estimator_\n",
    "top_score4 = gs_results4.best_score_\n",
    "top_parameters4 = gs_results4.best_params_\n",
    "\n",
    "# Display the gridsearch results\n",
    "gs4_df = gs_to_clean_df(search_results4, sort_by='rank_score')\n",
    "gs4_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a527ac41-6c0c-4432-9370-96e0c98b91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tree_num_distribution = [1150, 1200, 1250]\n",
    "max_depth = [12, 13, 14, 15]\n",
    "\n",
    "model_pipeline = Pipeline([(\"Tfidf_Vect\", TfidfVectorizer()),\n",
    "                           (\"boosted_trees\", LGBMClassifier(silent=False, random_state=42))])\n",
    "\n",
    "parameter_grid = [{'Tfidf_Vect__analyzer' : ['word'],\n",
    "                   'Tfidf_Vect__ngram_range' : [(1,2)],\n",
    "                   'Tfidf_Vect__max_features':[8761],\n",
    "                   'boosted_trees__n_estimators' : tree_num_distribution, \n",
    "                   'boosted_trees__max_depth' : max_depth,\n",
    "                   'boosted_trees__subsample' : [0.8, 0.9, 1.0],                    # Enable random selection of training cases (rows).\n",
    "                   'boosted_trees__colsample_bytree' : [0.8, 0.9, 1.0],             # Percentage of features to randomly consider at each split.\n",
    "                   'boosted_trees__learning_rate' : [0.008, 0.01, 0.12]}]\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=model_pipeline, param_grid=parameter_grid, scoring='accuracy', refit='accuracy', n_jobs=5, verbose=3)\n",
    "\n",
    "gs.fit(X, y)\n",
    "\n",
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs5.pkl'\n",
    "\n",
    "with open(PATH, 'wb') as file:\n",
    "    pickle.dump(gs, file)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f27034e-ddd4-49ba-8d74-5d3e386ac54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyzer</th>\n",
       "      <th>max_features</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>subsample</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13</td>\n",
       "      <td>1200</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13</td>\n",
       "      <td>1200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14</td>\n",
       "      <td>1150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word</td>\n",
       "      <td>8761</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>14</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analyzer max_features ngram_range colsample_bytree learning_rate max_depth  \\\n",
       "0     word         8761      (1, 2)              0.9          0.01        13   \n",
       "1     word         8761      (1, 2)              0.9          0.01        13   \n",
       "2     word         8761      (1, 2)              0.9          0.01        13   \n",
       "3     word         8761      (1, 2)              0.9          0.01        14   \n",
       "4     word         8761      (1, 2)              0.9          0.01        14   \n",
       "\n",
       "  n_estimators subsample  mean_score  rank_score  \n",
       "0         1200       0.8      0.8834           1  \n",
       "1         1200       0.9      0.8834           1  \n",
       "2         1200       1.0      0.8834           1  \n",
       "3         1150       1.0      0.8832           4  \n",
       "4         1150       0.9      0.8832           4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './data/Processed/increment_train_size/pickle/gbf_5000_gs5.pkl'\n",
    "\n",
    "# Read in the model file created by the above gridsearch.\n",
    "with open(PATH, 'rb') as file:\n",
    "    gs_results5 = pickle.load(file)\n",
    "    \n",
    "search_results5 = gs_results5.cv_results_\n",
    "top_estimator5 = gs_results5.best_estimator_\n",
    "top_score5 = gs_results5.best_score_\n",
    "\n",
    "# Display the gridsearch results\n",
    "gs5_df = gs_to_clean_df(search_results5, sort_by='rank_score')\n",
    "gs5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc1833cd-913e-4c75-9762-5791dfaabf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'boosted_trees__colsample_bytree': 1.0, 'boosted_trees__learning_rate': 0.05, 'boosted_trees__max_depth': 9, 'boosted_trees__n_estimators': 300, 'boosted_trees__subsample': 0.8}\n",
      "\n",
      "Test set score: 0.88264\n",
      "==================================================================================================\n",
      "\n",
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'boosted_trees__colsample_bytree': 0.8, 'boosted_trees__learning_rate': 0.08, 'boosted_trees__max_depth': 9, 'boosted_trees__n_estimators': 200, 'boosted_trees__subsample': 0.8}\n",
      "\n",
      "Test set score: 0.88132\n",
      "==================================================================================================\n",
      "\n",
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'boosted_trees__colsample_bytree': 1.0, 'boosted_trees__learning_rate': 0.1, 'boosted_trees__max_depth': 11, 'boosted_trees__n_estimators': 150, 'boosted_trees__subsample': 0.7}\n",
      "\n",
      "Test set score: 0.88252\n",
      "==================================================================================================\n",
      "\n",
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'boosted_trees__colsample_bytree': 1.0, 'boosted_trees__learning_rate': 0.01, 'boosted_trees__max_depth': 12, 'boosted_trees__n_estimators': 1200, 'boosted_trees__subsample': 0.8}\n",
      "\n",
      "Test set score: 0.88248\n",
      "==================================================================================================\n",
      "\n",
      "==================================================================================================\n",
      "Model params:\n",
      " {'Tfidf_Vect__analyzer': 'word', 'Tfidf_Vect__max_features': 8761, 'Tfidf_Vect__ngram_range': (1, 2), 'boosted_trees__colsample_bytree': 0.9, 'boosted_trees__learning_rate': 0.01, 'boosted_trees__max_depth': 13, 'boosted_trees__n_estimators': 1200, 'boosted_trees__subsample': 0.8}\n",
      "\n",
      "Test set score: 0.8818\n",
      "==================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters for each of the Gradient Boosted Decision Tree models. \n",
    "print_test_score(test_df, top_estimator1, top_parameters1)\n",
    "print_test_score(test_df, top_estimator2, top_parameters2)\n",
    "print_test_score(test_df, top_estimator3, top_parameters3)\n",
    "print_test_score(test_df, top_estimator4, top_parameters4)\n",
    "print_test_score(test_df, top_estimator5, top_parameters5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
